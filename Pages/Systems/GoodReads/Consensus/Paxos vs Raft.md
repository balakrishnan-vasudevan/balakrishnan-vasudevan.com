
- Have we reached consensus on distributed consensus?
**Type:** Conference Presentation
**Status:** Queued
**Link:** https://arxiv.org/pdf/2004.05074.pdf

State machine replication requires that an application’s deterministic state machine is replicated across n servers with each applying the same set of operations in the same order. This is achieved using a replication log, managed by a distributed consensus algorithm, typically Paxos or Raft.

We assume that the system is non-Byzantine [21] but we do not assume that the system is synchronous. Messages may be arbitrarily delayed and participating servers may operate at any speed, but we assume message exchange is reliable and in-order (e.g., through use of TCP/IP). We do not depend upon clock synchronisation for safety, though we must for liveness.

![[Pasted image 20250321124644.png]]

Candidate An active state where it is trying to become a leader using the RequestVotes RPC. Leader An active state where it is responsible for adding operations to the replicated log using the AppendEntries RPC.

Initially, servers are in the follower state. Each server continues as a follower until it believes that the leader has failed. The follower then becomes a candidate and tries to be elected leader using RequestVote RPCs. If successful, the candidate becomes a leader. The new leader must regularly send AppendEntries RPCs as keepalives to prevent followers from timing out and becoming candidates. Each server stores a natural number, the term, which increases monotonically over time. Initially, each server has a current term of zero. The sending server’s (hereafter, the sender) current term is included in each RPC. When a server receives an RPC, it (hereafter, the server) ﬁrst checks the included term. If the sender’s term is greater than the server’s, then the server will update its term before responding to the RPC and, if the server was either a candidate or a leader, step down to become a follower. If the sender’s term is equal to that of the server, then the server will respond to the RPC as usual. If the sender’s term is less than that of the server, then the server will respond negatively to the sender, including its term in the response. When the sender receives such a response, it will step down to follower and update its term.

One of the n servers is designated the leader. All operations for the state machine are sent to the leader. The leader appends the operation to their log and asks the other servers to do the same. Once the leader has received acknowledgements from a majority of servers that this has taken place, it applies the operation to its state machine. This process repeats until the leader fails. When the leader fails, another server takes over as leader. This process of electing a new leader involves at least a majority of servers, ensuring that the new leader will not overwrite any previously applied operations.

When a leader receives an operation, it appends it to the end of its log with the current term. The pair of operation and term are known as a log entry. The leader then sends AppendEntries RPCs to all other servers with the new log entry. Each server maintains a commit index to record which log entries are safe to apply to its state machine, and responds to the leader acknowledging successful receipt of the new log entry. Once the leader receives positive responses from a majority of servers, the leader updates its commit index and applies the operation to its state machine. The leader at any time a server can be in one of three states: Follower A passive state where it is responsible only for replying to RPCs.

Paxos. A follower will timeout after failing to receive a recent AppendEntries RPC from the leader. It then becomes a candidate and updates its term to the next term such that t mod n = s where t is the next term, n is the number of servers and s is the candidate’s server id. The candidate will send RequestVote RPCs to the other servers. This RPC includes the candidate’s new term and commit index. When a server receives the RequestVote RPC, it will respond positively provided the candidate’s term is greater than its own. This response also includes any log entries that the server has in its log subsequent to the candidate’s commit index. Once the candidate has received positive RequestVote responses from a majority of servers, the candidate must ensure its log includes all committed entries before becoming a leader. It does so as follows. For each index after the commit index, the leader reviews the log entries it has received alongside its own log. If the candidate has seen a log entry for the index then it will update its own log with the entry and the new term. If the leader has seen multiple log entries for the same index then it will update its own log with the entry from the greatest term and the new term. An example of this is given in Figure 2. The candidate can now become a leader and begin replicating its log to the other servers. Raft. At least one of the followers will timeout after not receiving a recent AppendEntries RPC for the leader. It will become a candidate and increment its term. The candidate will send RequestVote RPCs to the other servers. Each includes the candidate’s term as well as the candidate’s last log term and index. When a server receives the RequestVote request it will respond positively provided the candidate’s term is greater than or equal to its own, it has not yet voted for a candidate in this term, and the candidate’s log is at least as up-to-date as its own. This last criterion can be checked by ensuring that the candidate’s last log term is greater than the server’s or, if they are the same, that the candidate’s last index is greater than the server’s. Once the candidate has received positive RequestVote responses from a majority of servers, the candidate can become a leader and start replicating its log. However, for safety Raft requires that the leader does not update its commit index until at least one log entry from the new term has been committed. As there may be multiple candidates in a given term, votes may be split such that no candidate has a majority. In this case, the candidate times out and starts a new election with the next term.

then includes the updated commit index in subsequent AppendEntries RPCs. A follower will only append a log entry (or set of log entries) if its log prior to that entry (or entries) is identical to the leader’s log. This ensures that log entries are added in-order, preventing gaps in the log, and ensuring followers apply the correct log entries to their state machines.

Theorem 3.1 (State Machine Safety). If a server has applied a log entry at a given index to its state machine, no other server will ever apply a diﬀerent log entry for the same index.

Theorem 3.2 (Leader Completeness). If an operation op is committed at index i by a leader in term t then all leaders of terms > t will also have operation op at index i.

Understandability. Raft guarantees that if two logs contain the same operation then it will have the same index and term in both. In other words, each operation is assigned a unique index and term pair. However, this is not the case in Paxos, where an operation may be assigned a higher term by a future leader, as demonstrated by operations B and C in Figure 2b. In Paxos, a log entry before the commit index may be overwritten. This is safe because the log entry will only be overwritten by an entry with the same operation, but it not as intuitive as Raft’s approach. The ﬂip side of this is that Paxos makes it safe to commit a log entry if it is present on a majority of servers; but this is not the case for Raft, which requires that a leader only commits a log entry from a previous term if it is present on the majority of servers and the leader has committed a subsequent log entry from the current term. In Paxos, the log entries replicated by the leader are either from the current term or they are already committed. We can see this in Figure 2, where all log entries after the commit index on the leader have the current term. This is not the case in Raft where a leader may be replicating uncommitted entries from previous terms. Overall, we feel that Raft’s approach is slightly more un derstandable than Paxos’ but not signiﬁcantly so.

Eﬃciency. In Paxos, if multiple servers become candidates simultaneously, the candidate with the higher term will win the election. In Raft, if multiple servers become candidates simultaneously, they may split the votes as they will have the same term, and so neither will win the election. Raft mitigates this by having followers wait an additional period, drawn from a uniform random distribution, after the election timeout. We thus expect that Raft will be both slower and have higher variance in the time taken to elect a leader. However, Raft’s leader election phase is more lightweight than Paxos’. Raft only allows a candidate with an up-to-date log to become a leader and thus need not send log entries during leader election. This is not true of Paxos, where every positive RequestVote response includes the follower’s log entries after the candidate’s commit index. There are various options to reduce the number of log entries sent but ultimately, it will always be necessary for some log entries to be sent if the leader’s log is not already up to date. It is not just with the RequestVote responses that Paxos sends more log entries than Raft. In both algorithms, once a candidate becomes a leader it will copy its log to all other servers. In Paxos, a log entry may have been given a new term by the leader and thus the leader may send another copy of the log entry to a server which already has a copy. This is not the case with Raft, where each log entry keeps the same term throughout its time in the log. Overall, compared to Paxos, Raft’s approach to leader elec tion is surprisingly eﬃcient for such a simple approach.


||Paxos|Raft|ZAB|
|---|---|---|---|
|Semantics|Semantics: Paxos is fundamentally about reaching agreement in a network of unreliable components. It ensures that a single value is agreed upon among the participants, even if some of them fail.  <br>Intuition: Imagine a group of people trying to decide on a place to eat. They propose options and vote, ensuring they eventually agree on one place, even if some members are indecisive or change their minds. The key is that once a decision is made, it’s final.|Semantics: Raft’s primary goal is to simplify the consensus process. It breaks down the process into smaller subproblems.  <br>Intuition: Picture a classroom where students elect a class representative. This representative then takes decisions on behalf of the class, ensuring everyone’s on the same page. If the representative is absent, a new one is elected.|Semantics: ZAB is about total order broadcast. It ensures all actions are executed in the same order across all nodes.  <br>Intuition: Think of a series of tasks that need to be executed in a specific sequence. ZAB ensures that this sequence is maintained across all nodes, ensuring consistency.|
|Phases|Paxos operates mainly in two phases — Prepare and Accept. The Prepare phase is about proposing a value, and the Accept phase is about agreeing on it.|Leader Election: Raft has a unique approach to leader election. A new leader is elected whenever the current one fails. This ensures system availability.|Election Phase: A leader is elected. This is crucial for the subsequent phases.  <br>Discovery Phase: The leader establishes synchronization with all followers. This ensures that all nodes start from a consistent state.  <br>Broadcast Phase: The leader broadcasts client updates. This ensures that all nodes have the same data.|
|How does it ensure that each term has at most one leader?|A server s can only be a candidate in a term t if t mod n = s. There will only be one candidate per term so only one leader per term.|A follower can become a candidate in any term. Each follower will only vote for one candidate per term, so only one candidate can get a majority of votes and become the leader.||
|How does it ensure that a new leader’s log contains all committed log entries?|Each RequestVote reply includes the follower’s log entries. Once a candidate has received RequestVote responses from a majority of followers, it adds the entries with the highest term to its log.|A vote is granted only if the candidate’s log is at least as up-to-date as the followers’. This ensures that a candidate only becomes a leader if its log is at least as up-to-date as a majority of followers.||
|How does it ensure safely that leaders commit log entries from previous terms?|Log entries from previous terms are added to the leader’s log with the leader’s term. The leader then replicates the log entries as if they were from the leader’s term.|The leader replicates the log entries to the other servers without changing the term. The leader cannot consider these entries committed until it has replicated a subsequent log entry from its own term.||
||Multi-Paxos: An optimization where one node remains the leader until it fails, reducing the number of messages. This is especially useful in systems where leadership changes are costly.|Log Replication: The leader is responsible for log replication. It ensures that all logs are consistent across nodes.||
|Advantages|Proven correctness and wide applicability. It’s a foundation for many other consensus algorithms.|Its main advantage is its simplicity. It’s easier to understand and implement than Paxos.|Tailored for ZooKeeper’s needs, ensuring strong consistency.|
|Disadvantages|Its complexity makes it hard to implement and understand. Many systems opt for simpler algorithms due to this.|Some argue it’s not as generalized as Paxos. However, its simplicity often outweighs this disadvantage.|It’s specific to ZooKeeper’s use case. This might make it less suitable for other applications.|
|||||



| Paxos                                | Raft                                 |
| ------------------------------------ | ------------------------------------ |
| ![[Pasted image 20250321124950.png]] | ![[Pasted image 20250321125006.png]] |
| ![[Pasted image 20250321124929.png]] | ![[Pasted image 20250321125021.png]] |
| ![[Pasted image 20250321124911.png]] | ![[Pasted image 20250321125031.png]] |
| ![[Pasted image 20250321124847.png]] | ![[Pasted image 20250321125039.png]] |