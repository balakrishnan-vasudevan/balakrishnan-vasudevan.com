# Letâ€™s implement the log database or Big ideas behind the scene

Tags: databases
Category: Articles
Company: general
Status: Not started
URL: https://d9nich.medium.com/lets-implement-the-log-database-or-big-ideas-behind-the-scene-694d86bdc2a6

**In** most situations, you wouldnâ€™t create your own storage, but you shouldÂ **select**Â a storage engine, that matches your application needs. ToÂ **tune**Â a database to your data flow you need to knowÂ *how the storage engine works*. In this chapter, we will exploreÂ **log**Â database engines byÂ **building**Â our own from scratch. Prepare a coffeeâ˜• weâ€™re ready to diveğŸ¤¿.

[https://miro.medium.com/v2/resize:fit:1400/0*qlLZXotK8lP65LqO](https://miro.medium.com/v2/resize:fit:1400/0*qlLZXotK8lP65LqO)

Photo byÂ [Jandira Sonnendeck](https://unsplash.com/@jandira_sonnendeck?utm_source=medium&utm_medium=referral)Â onÂ [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

> â€œCleaning and organizing is a practice, not a project.â€ â€” proverb
> 

# **What database should do?**

The database should do two things:

1. When you give data, the database shouldÂ **save**Â data.
2. When you ask for stored data, the databaseÂ **gives it back**Â to you.

The worldâ€™s simplest database implemented in Python is provided belowğŸ‘‡:

Simple database solution

You can callÂ **`db_upsert`**Â to store a value in the database. It can be anything from a simple string to JSON. Then you can callÂ **`db_get`**Â to retrieve the most recent value byÂ **key**.

Example of program usage:

![https://miro.medium.com/v2/resize:fit:1400/1*dVUvrkVYxZPVHRuKXV5IXA.png](https://miro.medium.com/v2/resize:fit:1400/1*dVUvrkVYxZPVHRuKXV5IXA.png)

executionÂ **example**

Every time we need toÂ **add**Â a new key/value pair we put it at the end of the file. This file is also well known as aÂ **log**Â orÂ **append-only**Â data file.Â **Logs**Â are incredibly fast because appending to a file is generally very efficient. The modern database has more issues to deal with (*concurrency*Â access,Â *errors*Â in runtime), but the basic principle remains the same.

ToÂ **get**Â a value by key, we need to search for the latest occurrence in a file. This operation in our case has awful performance â€”Â **O(n)**. You have to search through all the files to find the newest value. It means that a 2 times bigger file, will take 2 times more time to find a value.

The solution to the â€œ**get**â€ problem in our situation is to add anÂ **index**. However, adding an index will slow down the database on write. This is an importantÂ **trade-off**Â in storage systems: every indexÂ **slows down**Â writes, but well-chosen indexesÂ **speed up**Â reads.

# **Indexes**

We will start with theÂ **hash**Â index for theÂ **key-value**Â store. Itâ€™ll be a good point to understand how we can implement a more complicated one.

Key-value databases are very similar to aÂ **dictionary**Â that is implemented in all widespread programming languages. We will use this structure for implementing our index of data on disk.

In our example application, we use an append-only strategy. We can store the position of the newest data in the file(**offset**) in our dictionary.

![https://miro.medium.com/v2/resize:fit:1400/1*8_BAG7TKJwubyHaYouLB7A.png](https://miro.medium.com/v2/resize:fit:1400/1*8_BAG7TKJwubyHaYouLB7A.png)

In memoryÂ **index**Â representation

â„¹ï¸: In the picture, the file is presented and divided by 30 bytes in each row, however in reality this is just one big unbreakable row.

Whenever youÂ **append**Â a new key value, you alsoÂ **update**Â the offset in your index. (Keep itÂ **up to date**). If you want to read data, you just look in your index and seek this position.

â„¹ï¸: Seek of position is very efficient and is equal toÂ **O**(1)

An example of my simple DB application with an index is shown belowğŸ‘‡:

DB with anÂ **index**Â on python

![https://miro.medium.com/v2/resize:fit:806/1*5W8qqBVoljQaj9PFD1YSZQ.png](https://miro.medium.com/v2/resize:fit:806/1*5W8qqBVoljQaj9PFD1YSZQ.png)

ExecutionÂ **index application**

For you, as a reader, this way to create an index may sound naive, but this approach is used in the real database â€”Â [Bitcask](https://en.wikipedia.org/wiki/Bitcask)Â ([Riak](https://en.wikipedia.org/wiki/Riak)Â storage engine). Bitcask requires that all of the availableÂ **keys**Â can fit in the memory. However,Â **values**Â can be larger thanÂ **keys**, because you can retrieve â€œvalueâ€ from the disk usingÂ **single seek**. Moreover, if the information has beenÂ **cached**Â by the filesystem, an application doesnâ€™t require any I/O operations at all.

## **Use cases for such storage**

The best example isÂ **counting**Â how many people have visited our certain page on the website. We have aÂ **limited**Â amount of pages (**keys**) and the value of our storage isÂ **frequently updated**.

## **Compaction ğŸ“¦**

Our tactic is to append changes to a single file, but in this scenario, we can easily at some point of time runÂ **out of disk space**. To avoid this we should:

1. Break the log into segments of a certain size
2. Regularly doÂ **compaction**

â˜ï¸: Each segment has aÂ **separate**Â in-memory hash table (index matching keys with values offset). In order to find the value of a key, we first search for a key in theÂ **latest**Â dictionary and return the value if we found. If the key is absent in the dictionary we move to theÂ **older**Â one.

**Compaction**Â â€” throw away outdated values, keeping only the recent one

Compaction advises:

- ğŸ”’Segments are never modified, so the compacted content is written to a new file
- ğŸªCompaction can be performed in theÂ **background**Â (in a separate thread), during serving read requests
- ğŸ§¹After compaction, we can deleteÂ **old**Â files

![https://miro.medium.com/v2/resize:fit:1400/1*VkYmnRFi8ZzmaBOxrluv4A.png](https://miro.medium.com/v2/resize:fit:1400/1*VkYmnRFi8ZzmaBOxrluv4A.png)

**Compaction**Â process

One more optimization is toÂ **merge**Â several data file segments duringÂ **compaction**Â because compaction often makes segments smaller (the key appears several times in the log).

![https://miro.medium.com/v2/resize:fit:1400/1*DvHXZPneMhe8SzpOBsjRRQ.png](https://miro.medium.com/v2/resize:fit:1400/1*DvHXZPneMhe8SzpOBsjRRQ.png)

**Compaction**Â andÂ **merging**Â process

â„¹ï¸: In the merge process we keep the number of segments small, so we check only several dictionaries, keeping the merge & compaction algorithm efficient.

## **Compaction common issues**

In theory, compaction is easy to achieve, however, there are some key moments that can slow down us:

- **âŒDeleting**Â records â€” if you want to delete a record you need to push a special record (*tombstone*). When compaction occurs and the latest value is aÂ *tombstone*, we willÂ **omit the**Â key in the compacted file.
- ğŸ”ƒ CrashÂ **recovery**Â â€” if the database is restarted we need to load ourÂ *in-memory hashes*. Of course, we can restore it by readingÂ **all**Â database files. However,Â [Bitcask](https://en.wikipedia.org/wiki/Bitcask)Â comes with an approach of storing snapshots of aÂ *hash map*, to speed up this process.
- âš¡Â **Partially**Â written records â€” The database canÂ **crash**Â halfway through writing a key/value pair to a file. Bitcask stores a checksum with a row and skips rows if the checksum doesn't match.
- ğŸ”€Â **Concurrency**Â â€” Itâ€™s common practice to keep one thread for writing and multiple threads for reading.
- ğŸ”¢**Format**Â â€”Â **CSV**Â in our implementation is presented only forÂ **learning purposes**. For a real system, itâ€™s better to useÂ **binary formats**. Binary formats give you an opportunity to omit the delimiter of rows: first, you encodeÂ *string length*Â and then provide aÂ *string by itself*. This way itâ€™s much faster and simpler.

ğŸ”—: More about formats you can read inÂ [my article](https://medium.com/@d9nich/json-xml-protobuf-thrift-avro-or-everything-you-need-to-know-about-encoding-data-6077a7e769e2).

## **Drawbacks and cons of an append-only log**

The attentive reader should ask a question: â€œWhy not update a record value in place?â€. Well, this opens the main advantages of LSM trees (Log-structured merge-tree)

AdvantagesğŸ‘:

- ğŸ’¿Appending and compaction/merging isÂ **faster**Â than the random write operation, especially if weâ€™re using HDD.
- âªItâ€™s easier to implementÂ **concurrent**Â access andÂ **recovery**. You donâ€™t need to worry about the situation when you write half of the new data, the application crashes and you getÂ **half new**Â andÂ **half old**Â data.

LimitationsğŸ‘:

- ğŸ§ All keys shouldÂ **fit**Â inÂ **memory**. Of course, you can keep the dictionary on disk, but it requires expensive I/O operations and hash collision complicated logic.
- ğŸ…°â†”ğŸ…±Â **Range**Â queries areÂ **not efficient**. To scan fromÂ **`foo001`**Â toÂ **`foo999`**Â youÂ **need to scan**Â each key individually.