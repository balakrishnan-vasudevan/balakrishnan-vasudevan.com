#uuid

Why do you need it?

A globally unique ID needs to be assigned for every event in a service. The auto-increment feature in any DB would do the job, but in a distributed DB, different nodes would independently generate these identifiers.

  

Requirements:

1. Uniqueness
    
2. Scalability
    
3. Availability
    
4. # of bits = 64 = 2^64 = 1.8446744073709551616 × 10^19 numbers are available, even with 1 billion events per day, per year # of events = 365 x 1x10^9  
    == 2^64/365 x 10^9 ⇒ 5x10^25 years
    

  
**

There can be millions of events happening per second in a large distributed system. Commenting on a post on Facebook, sharing a Tweet, and posting a picture on Instagram are just a few examples of such events. We need a mechanism to distinguish these events from each other. One such mechanism is the assignment of globally unique IDs to each of these events. 

1. Design of a Unique ID Generator: After enlisting the requirements of the design, we discuss three ways to generate unique IDs: using UUID, using a database, and using a range handler.
    
2. Unique IDs with Causality: In this lesson, we incorporate an additional factor of time in the generation of IDs and explain the process by taking causality into consideration.
    

  

Unique ID Generator:

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXd47Qbi8lzmSNF0XeFlrgROeGRbuwkCBcx6cp3T3jiaEmfIozHhDqs24EArbtjSgtMtrzq0o4h0MQkHElhEXrpMyeUj_y33dzL5W4JuURTAWElsys30orKGxtVolOW-4y3anjMKYB_UCanBdD_fMpYxceM?key=j4FcJWeNiYnhIt4RY32tSQ)

Approaches:

1. UUID:  
    Universally unique IDs  
    Each server can generate its own ID and assign the ID to its respective event. No coordination is needed for UUID since it’s independent of the server. Scaling up and down is easy with UUID, and this system is also highly available. Furthermore, it has a low probability of collisions. The design for this approach is given below:  
    ![](https://lh7-us.googleusercontent.com/docsz/AD_4nXeI0LyicAMsRSLymdx_fq3X9mal0_H9M8awQxU3sU_DwXMRUZ5eYXD4ILz7_eb0YJSA_xN0eUoms1yCIvzh7PrY7fUP4HP6TT84mJ4xswkMjrF_BeN1zdaVDDAzmfq_7NVx3OIKN0RkvM2NI8DZCegG9A?key=j4FcJWeNiYnhIt4RY32tSQ)  
    Doesn’t require sync between servers  
    Simple approach  
    Scalable  
    Available  
    Issues:  
    128-bit numbers makes primary key indexing slower, so slower inserts  
    UUIDs are not deterministically unique  
    Chance of duplication  
    2. Use a Database:  
    Use the auto-increment feature of a database. Central db provides a current id and then increments values by one. Current ID is used as the unique identifier.  
    ![](https://lh7-us.googleusercontent.com/docsz/AD_4nXfauX95-Zz2pRTiPFtxJKLIobpUKmrl-57vD0JnlX9kL8LfJoYt8xMiY_fjo1cwgXCZH1-_Sjhka_dTtellUhJF2YoFqZvAhfKE3s2Mp8ZfyBo7hEnYVX0AkpxYUFxXS9mz4RGVKPxBlbq-KhX2QmnWgXY?key=j4FcJWeNiYnhIt4RY32tSQ)  
      
    

Central DB can be a Single Point of Failure. Add a LB and use this as a service.  
![](https://lh7-us.googleusercontent.com/docsz/AD_4nXemjl67dUP2tZORyliun9du3SYMZXXFWWCP1ueOygdQTkKW77Aa-dblJuscTTNBV9x11KnfzEJoQZAAuh_y8sESbggP6SM5QS-ejD4ZzC5vTde3U0le7Vr1JcBJ90PWg89dV2rbPx3Hm7vGfl7SgaDW-AA?key=j4FcJWeNiYnhIt4RY32tSQ)

  

Unique IDs with causality:

Some applications need the events to have unique identifiers and carry any relevant causality information. An example of this is giving an identifier to the concurrent writes of a key into a key-value store to implement the last-write-wins strategy.

### Use UNIX timestamps

UNIX time stamps are granular to the millisecond and can be used to distinguish different events. We have an ID-generating server that can generate one ID in a single millisecond. Any request to generate a unique ID is routed to that server, which returns a time stamp and then returns a unique ID. Our system works well with generating IDs, but it poses a crucial problem. The ID-generating server is a single point of failure (SPOF), and we need to handle it. To cater to SPOF, we can add more servers. Each server generates a unique ID for every millisecond. To make the overall identifier unique across the system, we attach the server ID with the UNIX timestamp. Then, we add a load balancer to distribute the traffic more efficiently.

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXdnV6E_-FRyvDBCm0SFZJ_PJswtehdwXiZRwKxSnEnRNPp9pVHqb3tC_d--4JzuaNetgCRlOuqqRPH1jYpVancA6lz3g2vu90yEUI8y1WkKVVh6eLcFP6VhR5lIMg40nMvYgMoiym0YlXNfiB1r8nbVIf4?key=j4FcJWeNiYnhIt4RY32tSQ)

  

For two concurrent events, the same time stamp is returned and the same ID can be assigned to them. This way, the IDs are no longer unique.

  

In order to avoid single point of failure, use the number of servers and add that to the value being generated by the DB  
![](https://lh7-us.googleusercontent.com/docsz/AD_4nXd1RTzdml5asUyRajK4sNwxRDf288r_SaqlLmhgx07vBtf-bDDKDB2KZoWHdgVQhzYo44H_pp8JzDGO6dDIyC3esdCZ-4FqZTw6MqOpfIdzIjYt2lXmvy426UdqZMf1yOZWpgVGn06sRs5sC74gS2235w?key=j4FcJWeNiYnhIt4RY32tSQ)

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXdOzDHoNu4GmkJf_5RF87-8I4Ld13C1dNNPJ7CV4hCM2Q51ndFgeiCPkks540YPcI3rtDiFAN0No9RRxXjJxutZNTk8AIGU8E0cP-W6M6Gs-1nksufzBqHzrWr3S8V1VdN5WyE_aIDdEIzO-6M7ZdcDzoY?key=j4FcJWeNiYnhIt4RY32tSQ)

  

Though this method is somewhat scalable, it’s difficult to scale for multiple data centers. The task of adding and removing a server can result in duplicate IDs. For example, suppose m=3, and server A generates the unique IDs 1, 4, and 7. Server B generates the IDs 2, 5, and 8, while server C generates the IDs 3, 6, and 9. Server B faces downtime due to some failure. Now, the value m is updated to 2. Server A generates 9 as its following unique ID, but this ID has already been generated by server C. Therefore, the IDs aren’t unique anymore.

  

1. Use a range handler  
    We can use ranges in a central server. Suppose we have multiple ranges for one to two billion, such as 1 to 1,000,000; 1,000,001 to 2,000,000; and so on. In such a case, a central microservice can provide a range to a server upon request.
    

Any server can claim a range when it needs it for the first time or if it runs out of the range. Suppose a server has a range, and now it keeps the start of the range in a local variable. Whenever a request for an ID is made, it provides the local variable value to the requestor and increments the value by one.

Let’s say server 1 claims the number range 300,001 to 400,000. After this range claim, the user ID 300,001 is assigned to the first request. The server then returns 300,002 to the next user, incrementing its current position within the range. This continues until user ID 400,000 is released by the server. The application server then queries the central server for the next available range and repeats this process.

This resolves the problem of the duplication of user IDs. Each application server can respond to requests concurrently. We can add a load balancer over a set of servers to mitigate the load of requests. We use a microservice called range handler that keeps a record of all the taken and available ranges.

  
  

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXcJyHyOSu8xos3m4-_TJxGFDY9Cm7HuiEVY0OwT-6jMviPPyX7j3NyEKhKBmX98L16nPFh8JBZHE0igPeLTGImWmKfvdqzgYb0DBbasEqVbpXVP7AKcPJm7izZ3ySsQK4NHi5A5zG-ToQxPodjol00hkN8?key=j4FcJWeNiYnhIt4RY32tSQ)

We lose a significant range when a server dies and can only provide a new range once it’s live again. We can overcome this shortcoming by allocating shorter ranges to the servers, although ranges should be large enough to serve identifiers for a while.

  

Snowflake:

Twitter’s system

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXfVhJCGcyzmxxsre-VZro_jCiv1bU9Tk1g222WUm1pVwPjkYdqP9iW7-3IEzqoaaHWrHjBXr8sBya0v-7u45PoFB7eMVOMkVHSNcurXjUhKpjG5ICuo710itclpIoGVn4gjSsnOL98QodpzxRjb7lY41Zc?key=j4FcJWeNiYnhIt4RY32tSQ)

Sign bit: A single bit is assigned as a sign bit, and its value will always be zero. It makes the overall number positive. Doing so helps to ensure that any programming environment using these identifiers interprets them as positive integers.

• Time stamp: 41 bits are assigned for milliseconds. The Twitter Snowflake default epoch will be used. Its value is 1288834974657

1288834974657, which is equivalent to November 4, 2010, 01:42:54 UTC.

• Worker number: The worker number is 10 bits. It gives us 

2^10 = 1,024 worker IDs. The server creating the unique ID for its events will attach its ID.

• Sequence number: The sequence number is 12 bits. For every ID generated on the server, the sequence number is incremented by one. It gives us 

2^12 = 4,096 unique sequence numbers. We’ll reset it to zero when it reaches 4,096. This number adds a layer to avoid duplication.

  

### Pros

Twitter Snowflake uses the time stamp as the first component. Therefore, they’re time sortable. The ID generator is highly available as well.

### Cons

IDs generated in a dead period are a problem. The dead period is when no request for generating an ID is made to the server. These IDs will be wasted since they take up identifier space. The unique range possible will deplete earlier than expected and create gaps in our global set of user IDs.

  

The physical clocks are unreliable. For such clocks, the error can be 17 seconds per day. If we measure time using these on a server, the time drifts away.

Considering a single server, we won’t be affected by the drifting away of time since all transactions land on a single server. But in a distributed environment, the clocks won’t remain synced.

Due to the unreliability of measuring accurate time, no matter how often we synchronize these clocks with each other or other clocks with accurate measurement methods, there will always be skew between the various clocks involved in a distributed system.

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXcPzYeDp_XclRDNQa_6IqTyKmEJTrcQbHvcyEWLSDzoga1R4rIohp5PPVwMQFo1weU9tB_vanaC9N6THeeLCuZ7VNmqjyLH55L8GB8UBfHzdL5gvX8QagAihbCg_y1yuLn7ScwFuQAMLrGb1K5vPj6EXhQ?key=j4FcJWeNiYnhIt4RY32tSQ)

Using Logical Clocks:

Lamport Clocks

In Lamport clocks, each node has its counter. All of the system’s nodes are equipped with a numeric counter that begins at zero when first activated. Before executing an event, the numeric counter is incremented by one. The message sent from this event to another node has the counter value. When the other node receives the message, it first updates its logical clock by taking the maximum of its clock value. Then, it takes the one sent in a message and then executes the message.

Lamport clocks provide a unique partial ordering of events using the happened-before relationship. We can also get a total ordering of events by tagging unique node/process identifiers, though such ordering isn’t unique and will change with a different assignment of node identifiers. However, we should note that Lamport clocks don’t allow us to infer causality at the global level. This means we can’t simply compare two clock values on any server to infer happened-before relationship. Vector clocks overcome this shortcoming.

### Vector clocks

Vector clocks maintain causal history—that is, all information about the happened-before relationships of events. So, we must choose an efficient data structure to capture the causal history of each event.

Consider the design shown below. We’ll generate our ID by concatenating relevant information, just like the Twitter snowflake, with the following division:

- Sign bit: A single bit is assigned as a sign bit, and its value will always be zero.
    
- Vector clock: This is 53 bits and the counters of each node.
    
- Worker number: This is 10 bits. It gives us 2^{10} = 1,024 worker IDs.
    

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXee-vCcgMzVqSg_zgAvbpivwf2QYsfysDUi-p2QE5OJLqgDj4ipusEdRk_qwp3k3-_A8xOYttOWW0GZ1OAba2tcJj_KhsPKKa4r7vNABC4cVeh8ZBczfMCfWzQxbFBe1TbYxKj-2quhFbNrZsgIvi0Zc6A?key=j4FcJWeNiYnhIt4RY32tSQ)

Our approach with vector clocks works. However, in order to completely capture causality, a vector clock must be at least n nodes in size. As a result, when the total number of participating nodes is enormous, vector clocks require a significant amount of storage. Some systems nowadays, such as web applications, treat every browser as a client of the system. Such information increases the ID length significantly, making it difficult to handle, store, use, and scale.

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXdTX5k7n6gjIe5MKfqU6SYMXvGuSECUa_2oUi_zzi7E_Tm3X5oBXhlOzIfAG7BONqJgArnqntOTV3S0jaTaBFzmN1Ow9IbPXcwAFZYFFfy8Yderyx90KIUhMOcmuTqEz2yktKanrmZqpPHxHO-xXpQKisc?key=j4FcJWeNiYnhIt4RY32tSQ)

### TrueTime API

Google’s TrueTime API in Spanner is an interesting option. Instead of a particular time stamp, it reports an interval of time. When asking for the current time, we get back two values: the earliest and latest ones. These are the earliest possible and latest possible time stamps.

Based on its uncertainty calculations, the clock knows that the actual current time is somewhere within that interval. The width of the interval depends, among other things, on how long it has been since the local quartz clock was last synchronized with a more accurate clock source.

Google deploys a GPS receiver or atomic clock in each data center, and clocks are synchronized within about 7 ms. This allows Spanner to keep the clock uncertainty to a minimum. The uncertainty of the interval is represented as epsilon.

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXdMH_vn0gM_4dsTfUe77FzerR5fX7uJ2sysXaaafHcAWzGXXoU2RxnN5nIsuhEuLTfDq1AX1QI2J5HHRqlrMoenRyib-akujZcS4cIy_OQuMK46bx9Shkpo-7_sN5SR9ig6oWmCvtKlKRb3tak_dsPNWgU?key=j4FcJWeNiYnhIt4RY32tSQ)

![](https://lh7-us.googleusercontent.com/docsz/AD_4nXc2E3aqpQ7Ls_mOVn7pP8HswdD40BZ6ACDEhghnxfRkk8wD97V3NhpMR1GKxpoNDwfCFKzqiGqQHKAmnUBPMm-wbVWyg-YLMP8TP_n3WgXPLng_pfzuxu41rHZlCIukDrW8Pkifwpy2s76rxRyxkIYoqYM?key=j4FcJWeNiYnhIt4RY32tSQ)

Note: Globally ordering events is an expensive procedure. A feature that was fast and simple in a centralized database (auto-increment based ID) becomes slow and complicated in its distributed counterpart due to some fundamental constraints (such as consensus, which is difficult among remote entities).

For example, Spanner, a geographically distributed database, reports that “if a read-update transaction on a single cell (one column in a single row) has a latency of 10 milliseconds (ms), then the maximum theoretical frequency of issuing of sequence values is 100 per second. This maximum applies to the entire database, regardless of the number of client application instances, or the number of nodes in the database. This is because a single node always manages a single row.” If we could compromise on the requirements for global orderings and gapless identifiers, we would be able to get many identifiers in a shorter time, that is, a better performance.

**