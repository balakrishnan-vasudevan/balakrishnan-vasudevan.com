# Some Observations On the Messy Realities of Incident Reviews

Tags: incident-mgmt
Category: Articles
Company: general
Status: Not started
URL: https://www.adaptivecapacitylabs.com/2019/06/17/some-observations-on-the-messy-realities-of-incident-reviews/

**1) Incident reviews serve multiple purposes.** Some of these purposes are overt and explicit but many are not. Some of these purposes cut across others in unproductive ways. Competing agendas reveal the power dynamics present in the organization. A manager who complains that too few action items were produced has revealed his/her real interest: the reduction of an incident to a manageable discrete list of things to be done. See [here](https://www.adaptivecapacitylabs.com/blog/2018/10/08/the-multiple-audiences-and-purposes-of-post-incident-reviews/).

**2) The ostensible rationale for incident reviews is always incomplete and often misleading.** Although virtually every organization’s leadership claims to be vitally concerned with learning from incidents, the structure and results of incident reviews tend to align with the real demands. The most common demand from management is the restoration of the perceived *status quo ante* so that comfort returns and “normal” work can be resumed. Especially for high-consequence events, assertions of authority and the appearance of deliberate, incisive action are essential demonstrations of management control. It is not uncommon for high-consequence events to be handled mostly *in camera* and for the open, formal incident review to take on the flavor of “incident theater”.

**3) People learn different things from incident reviews** (analysis and meetings). It would be strange if this were not so. The learning includes specific details of technical processes and many other things. Mental models of [BelowTheLine](https://www.adaptivecapacitylabs.com/blog/2019/01/30/human-cognitive-work-happens-above-the-line/) vary widely. Incident reviews are opportunities to enrich and calibrate individual models and to promote model correlation across individuals.

**4) The learning is seldom narrowly technical.** As the process is inherently a *social* one, people learn how the organization responds to events, what tensions exist (between individuals, between teams, between management and workers, etc.), which topics are acceptable for discussion and which ones are forbidden, how power dynamics play out, how productive and unproductive management can be, and so forth. People learn these things whether or not those in charge want them to. *[Side note: people almost always recognize the real (as opposed to stated) reasons that these events play out along certain paths.]* See [“Social issues in post-mortems” in the Stella Report](https://snafucatchers.github.io/#4_1_2_Social_issues_in_postmortems).

**5)** Although written summaries of review meetings are common, **it is *impossible* to capture what is learned from the experience.** The incident review is a “live-action” event and even high quality video recording cannot recover the dynamic in the room. Choosing themes to discuss, handling the diversions and conflicts that arise, and engaging the individuals present are aspects of facilitation that depend heavily on the skill of the facilitator, skill that must necessarily be exercised in real-time.

**6)** We take it that one purpose of post-incident review could be **to create as rich an opportunity for learning as possible** so that every person attending can learn one or more things *relevant to them and their work*. The diversity of learning being unavoidable, this seems to be a good goal. This goal will necessarily be constrained by the conflicting requirements (see #1 and # above) and managing these is another demand on the facilitator.

**7) It’s a marathon, not a sprint.** We believe that creating and sustaining a rich learning environment requires commitment and effort over a much longer period than many appreciate — at minimum something like a year of consistent and deliberate engagements. Many organizations have driven post-incident reviews to become pallid, vapid, mechanical exercises whose value is limited to producing a defensible argument that management is occurring. Repairing this damage takes time.

**8) There are other units of analysis.** Most organizations treat a single incident as the unit of analysis — the incident is evaluated and countermeasures devised based on that evaluation. But *groups of incidents* can be compared and contrasted in order to extract themes and opportunities that are only dimly seen in the individual incident. We seldom find any such activity present in organizations we visit. This is not, of course, surprising given the sterile, myopic approach to the individual incident reviews!

![https://www.adaptivecapacitylabs.com/wp-content/uploads/2019/11/ACL-white-board.jpeg](https://www.adaptivecapacitylabs.com/wp-content/uploads/2019/11/ACL-white-board.jpeg)

Quite often, we will talk to new clients about what “learning from incidents” means to them. The descriptions we get in response tend to be important signals for us as we work through whatever project we’re engaged with.

Invariably, people will mention signals that *learning* is taking place in the form of what we might call [shallow data](https://www.adaptivecapacitylabs.com/blog/2018/03/23/moving-past-shallow-incident-data/), such as:
- A decrease in the *frequency* of incidents they experience
- A decrease in the *impact* incidents have on the business’ goals
- The *length of time* incidents last (or “time to resolve” or “time to detect” or “time to X”) will go down
- The number of completed post-incident “action” items will go up or the length of time it takes to complete them goes down
  
  Of course these aren’t indications of *learning* *per se,* they represent fluctuations of data that don’t represent much of the substance of what makes handling incidents difficult or the origins of where they come from.
  
  What we can say definitively is that learning is happening, regardless of what happens in post-incident activities. People are always learning. The challenge is not getting people to learn. The problem is making it easy for people to learn what is likely to be useful in the near term and the more distant future.
  
  Detecting that effective “learning” from an incident has taken place is quite difficult to do. Making progress in *learning from incidents* is difficult to capture and characterize. However, there are a number of potential indicators that, taken together, could provide evidence of progress in learning from incidents.
  
  These markers or indicators include:
  
  **More people will decide to attend post-incident review meetings.** Meeting attendance will grow. Engineers will report that they learn things about their systems there (and in the incident analysis write-ups that result) that they can’t anywhere else.
  
  **Post-incident review meeting attendance will include people from engineering and customer support** not directly involved in the incident under discussion.
  
  **Engineers will actively seek focused incident analysis training.** They will express interest in topics related to accident investigation and read more on these topics on their own time.
  
  **Tools that aid incident analysis and post-incident review meeting preparation, or enrich the post-incident artifacts will appear and be refined.**
  
  **The number of “orphan” post-incident “action items” (in JIRA or other task-tracking systems) will trend downward.** Orphan items will be “adopted” by being reviewed and cross-referenced to incidents and post-incident analysis write-ups.
  
  **Post-incident analysis document content will become richer** (e.g. include diagrams drawn by participants in post-incident review meetings, the actual transcripts of the incident response and handling, contributions from customer support staff).
  
  **The number of unique readers of post-incident analysis write-ups will grow over time.** Even months after the analysis is published there will be new views of the document(s). Comments, replies, highlights, tags, and other metadata regarding the content will come from an ever broader audience and spark new dialogue between readers.
  
  **Incident analysis documents will be used in new-hire onboarding or training** as vehicles to describe in rich detail the histories of involved technologies, the challenges and risks faced by teams, and configuration of systems and dependencies.
  
  **Incident analysis document content will be written and organized to make the incident features (sources, conditions, difficulties in handling, etc.) explicit enough that future readers will be able to easily find and understand them.** There will be regular evaluation of past incident analysis documents that confirm this.
  
  **Engineering teams will use incident analysis documents as primary training materials.**
  
  **Explicit references to specific incident analysis documents will appear more frequently in company internal documents.** Citations of specific incidents in project/product “roadmap” documents, “runbooks”, hiring plans, new systems design proposals, etc., are evidence that the authors understand both the value and the relevance of experience with incidents.
  
  **Incident analysis documents originating in engineering groups will routinely be reviewed by those in other groups** (such as customer support). Comments from these groups will be included and cross-referenced in the post-incident documents.
  
  **Post-incident documents originating in other groups (such as customer support) will routinely be reviewed by engineering groups.**
  
  We understand that these phenomena above can sound unbelievable to some, but we have seen them. At one time, the idea of deploying to production multiple times per day also sounded unbelievable. Until it wasn’t.
  

  

  
  Given that this is the holiday season in the US and both “Black Friday” and “Cyber Monday” are coming up, we thought we’d give some more detail on what doing an Aftermath project entails.
  
  We sincerely hope *not* to hear from you!
- ### **What is an Aftermath project?**
  
  An [“Aftermath](https://www.adaptivecapacitylabs.com/aftermath-projects/)” project is an immediate investigation and analysis of a significant incident.
  
  An Aftermath project provides senior management with a calibrated, independent review of the incident and the response. Aftermath projects ‘work’ because ACL is neutral, independent, and highly qualified to investigate, analyze, and produce and share productive perspectives and understandings.
  
  An Aftermath project is intense, exciting and different than [other projects](https://www.adaptivecapacitylabs.com/#services) we tend to do more often. Major incident aftermath is difficult and even dangerous period. But, as the saying goes, “This ain’t our first rodeo!” Our goal is to help the client company understand the incident and response in productive ways.
- ### **Why would a company ask ACL to come in and do such an analysis?**
  
  1. to demonstrate to stakeholders (such as investors, board members, partners, customers, etc.) that leaders are **taking the event seriously.**
  2. to provide an independent, explicit, technical account of the event and response;
  3. to reduce the tensions that flow from high-impact events;
  4. to prepare for additional reviews and follow-on decision making;
  
  In some cases, clients ask us to do an aftermath analysis because they believe stakeholders (investors, board members, etc.) are skeptical that they’d be able to do a deep and productive analysis *themselves.* On one occasion, we heard the client was told rather bluntly (paraphrasing) *“Why should we trust you to do the follow-up investigation when you were the ones who created the incident in the first place?”* The client mentioned that the fact that we are an external (neutral, independent) group was just as important as the quality of the analysis.
- ### **What is the product of the analysis?**
  
  1. The tangible products are: 
    1. a written report and
    2. a presentation to management describing the results of the analysis.
  2. The intangible products are:
	- immediate action that resonates with stakeholders, who are typically the primary audience for the results;
	- recognition of the company’s determination to obtain an independent evaluation of the incident and response;
	- interaction with operators, engineers, and managers that bolsters the company’s expressed interest in a thorough and deliberate analysis;
	- the visible, palpable presence of the ACL team on site.
- ### **How long does it take?**
  
  Aftermath projects take about **3 weeks**: 2 weeks for data collection and analysis, 1 week for synthesis, writing, and presentation.
- ### **Where does it happen?**
  
  Aftermath projects are conducted on-site or remotely. ACL personnel are present throughout the project, virtual or on-site.
  
  What is the startup process? How do we get going?
  
  1. 
    1. A company senior manager contacts ACL via email or phone to set up a meeting.
    2. The senior manager provides the company’s Non-Disclosure Agreement to ACL and the NDA is executed.
    3. ACL and company senior managers have a short videoconference that reviews the incident and immediate concerns.
    4. ACL proposes and company accepts a formal engagement.
    5. ACL personnel travel to the site and begin work.
- ### **What sort of event is suitable for an Aftermath project?**
  
  1. 
    1. The event occurred in the past 24 to 72 hours and the engagement will start within 24 to 48 hrs; an older event or delay in startup is likely to make Aftermath work impractical (more on this below);
    2. The event took place in an IT system, subsystem, or supporting system;
    3. The event involves a response process with IT features;
    4. The event has generated a substantial loss OR a repeat of the event would likely generate substantial loss;
    5. The event is not a crime.
- ### **What are the obstacles?**
  
  1. Organizational “drag” in approval:
	- ***Typical procurement processes don’t move quickly, even for a 2-3 week project.***
	- Approval and endorsement from the highest company authority is essential. Without this approval it is difficult to obtain cooperation at all levels of the organization.
	  2. Entanglement with other investigations: parallel regulatory or legal processes may make an Aftermath project infeasible.
	  3. Cost is usually not an obstacle. The incident will already have generated much greater costs and likely threatens to do more damage.
	  
	  If we can’t begin the analysis within days of the event, we’re not likely to be able to do the project. Why? Most importantly, the later we show up, the more the memories of individual perspectives will have coalesced into a single “party line” story. *If we begin our analysis and everyone we speak with gives us (roughly) the same story, it’s a signal that we’re too late to obtain authentic recall.*
	  
	  People talk to each other, especially right after significant events. This *immediate sense-making* takes place among those responding to the event. As time passes, the impact of the event and its consequences become clearer. Agendas and cross-currents begin to modify the story of the incident. Blame attaches to some people, groups, or parts of the org. Counterfactuals (“If only X had not been broken this would never have happened”) begin to dominate the interpretation of the event. Different perspectives harden into fixed accounts — often incomplete or even incorrect but nonetheless satisfying — that vie for attention. Peripheral issues begin to join as stakeholders recognize the potential of the incident as a motive force.
	  
	  In parallel, work continues to “clean up” the aftermath. Customers or partners are contacted. Contractual obligations and their consequences are evaluated and plans generated to deal with them. Over time, stories about the *origins* of the incident are simplified and become narrow as the organization struggles to free itself of the fear, shame, and anxiety that the event produced.
	  	  
	  A [question today on Twitter](https://twitter.com/DivineOps/status/1214944961587826688) regarding the distinction between “how” questions and “why?” questions reminded me of another reflection on this topic, in [*Tricks of the Trade: How to Think About Your Research While You’re Doing It](https://www.barnesandnoble.com/w/tricks-of-the-trade-howard-s-becker/1100617749).*
	  
	  Howard Becker (University of Chicago) in 1998 wrote on page 58:
- ### **Ask “How?” Not “Why?”**
  
  Everyone knows this trick. But, like many other things everyone knows, the people who know it don’t always use it when they should, don’t follow the prescription to ask how things happened, not why they happened. Why people do that is an interesting problem, though I suppose this sentence contains the answer: it seems more natural to ask why, as I just did. Somehow “Why?” seems more profound, more intellectual, as though you were asking about the deeper meaning of things, as opposed to the simple narrative “How?” would likely evoke. This prejudice is embodied in the old and meretricious distinction, invariably used pejoratively, between explanation and “mere” description.
  
  I first understood that “How?” was better than “Why?” as a result of doing field research. When I interviewed people, asking them why they did something inevitably provoked a defensive response. If I asked someone why he or she had done some particular thing I was interested in—”Why did you become a doctor?” “Why did you choose that school to teach at?” — the poor defenseless interviewee understood my question as a request for a justification, for a good and sufficient reason for the action I was inquiring about. They answered my “Why?” questions briefly, guardedly, pugnaciously, as if to say, “OK, buddy, that good enough for you?”
  
  When, on the other hand, I asked how something had happened—“How did you happen to go into that line of work?” “How did you end up teaching at that school?” — my questions “worked” well. People answered at length, told me stories filled with informative detail, gave accounts that included not only their reasons for whatever they had done, but also the actions of others that had contributed to the outcome I was inquiring about. And, when I interviewed marijuana users in order to develop a theory of the genesis of that activity, “How did you happen to start smoking grass?” evoked none of the defensive, guilty reaction evoked (as though I had accused them of something) by “Why do you smoke dope?”
  
  Why does “How?” work so much better than “Why?” as an interview question? Even cooperative, non-defensive interviewees gave short answers to “Why?” hey understood the question to be asking for a cause, maybe even causes, but in any event for something that could be summarized briefly in a few words. And not just any old cause, but the cause contained in the victim’s intentions. If you did it, you did it for a reason. OK, what’s your reason? Furthermore, “Why?” required a “good” answer, one that made sense and could be defended. The answer should not reveal logical flaws and inconsistencies. It should be socially as well as logically defensible; that is, the answer should express one of the motives conventionally accepted as adequate in that world. In other words, asking “why?” asks the interviewee for a reason that absolves the speaker of any responsibility for whatever bad thing’s occurrence lay behind the question. “Why are you late for work?” clearly asks for a “good” reason; “I felt like sleeping late today” isn’t an answer, even though true, because it conveys an illegitimate intention. “The trains broke down” might be a good answer, since it suggests that the intentions were good and the fault lay elsewhere (unless “You should leave early enough to take account of that possibility” lies in wait as a response). “It was foretold in my horoscope” will not do the trick in many places.
  
  “How?” questions, when I asked them, gave people more leeway, were less constraining, invited them to answer in any way that suited them, to tell a story that included whatever they thought the story ought to include in order to make sense. They didn’t demand a “right” answer, didn’t seem to be trying to place responsibility for bad actions or outcomes anywhere. They signaled idle or disinterested curiosity: “Gee, what happened on the way to work that made you so late?” hey didn’t telegraph the form the answer had to take (in the case of “why,” a reason contained in an intention). As a result, they invited people to include what they thought was important to the story, whether I had thought of it or not. You might not welcome an interviewee having that sort of freedom if you were doing a certain sort of research. If you wanted to get everyone to choose answers to your questions from the same small number of choices (as is sometimes, but not necessarily, the aim in survey research), so that you could count how many had chosen each, you wouldn’t want to hear about possibilities not contained in your list; those would have to go under “other” and couldn’t be used to do anything you had in mind to do.
  
  But the kind of research I was doing, and still do, was after something else. I wanted to know all the circumstances of an event, everything that was going on around it, everyone who was involved. (“All” and “everything” here are hyperbolic; I wouldn’t really want all that, but certainly a lot more than social scientists often do.) I wanted to know the sequences of things, how one thing led to another, how this didn’t happen until that happened. And, further, I was sure that I didn’t know all the people and events and circumstances involved in the story. I expected to keep adding to that collection, and making my understanding, my analysis, more complicated, as I learned from the people I talked to. I wanted to maximize their freedom to tell me things, especially things I hadn’t thought of.
  
  There’s an important exception to my condemnation of “why” questions. Sometimes researchers want to know, exactly, what kinds of reasons people give for what they have done or think they might do. When Blanche Geer and I interviewed medical students (Becker et al. [1961] 1977, 401–18) about the choices they intended to make of medical specialties—since they were still students, these choices were all hypothetical—what we wanted to know was, precisely, the kinds of reasons they would give for their choices. We wanted to chart the framework of acceptable reasons for choosing and the way those choices mapped onto the range of available specialties. We didn’t expect these choices to predict the choices students would actually make when, in the future, they entered one or another specialty. We wanted to know their reasons as part of our description of the perspective that guided their thinking while they were in school.
  
  So, in the field, you learn more from interview questions phrased as “how” than from those phrased as “why.” Effectiveness as an interview strategy does not warrant an idea’s theoretical usefulness. Still, it’s a clue.
  
  ![https://www.adaptivecapacitylabs.com/wp-content/uploads/2020/01/resilience-of-bone-talk.png](https://www.adaptivecapacitylabs.com/wp-content/uploads/2020/01/resilience-of-bone-talk.png)
  
  At the [Re-Deploy conference](https://re-deploy.io/) in 2019, our colleague Dr. Richard Cook gave one of the most cogent and clear talks on descriptions of resilience and resilience engineering that I’ve yet to come across. This work is only represented in this conference talk form — Richard did not capture this exceptional description in an article or written form [before he died in 2022](https://www.adaptivecapacitylabs.com/2022/09/12/richard-cook-a-life-in-many-acts/).
  
  I’m embedding the video below, but I’m also including an interactive transcript for those who prefer reading rather than watching and listening.
  
  If you’ve at all been interested in resilience and Resilience Engineering, I really do recommend spending the time to see this.
  
  
  We can think about the current unfolding epidemic as a case at scale of the general systems issues we have been thinking/writing… about for more than a decade. We are part of a multi-national scale ***natural laboratory*** on managing a spreading disruption relative to saturating critical points in the response system.
  
  Looking at this analysis from March 10 (attempt at visual analytics of the situation in progress):
  
  [https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca](https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca)
  
  The US looks like it will be the contrast for smaller nations that have kept a lid on the epidemic, especially since we are not testing at scale. We will see how much social distancing alone reduce can peaks in hospital/ICU load/overload.
  
  And this is not an episode to be handled and then be done with — but rather a process as a novel virus becomes endemic around the world with a time course of fluctuations, change (the virus itself) and responses as it becomes a part of the diseases of the world.
  
  The anticipation paradox is quite evident as COVID-19 outbreaks start to roll across the world, that is, effective counters require action in advance of the direct experience of the tangible consequences of harm but the ability to engage/mobilize/generate the needed response mechanisms can be limited without tangible harm (US’s inability to get large scale testing going). The delays (slow and stale) only serve to make later responses more disruptive and less effective. Anticipation requires acting that that builds future responsiveness without waiting for definitive information. The current situation highlights that the drivers of how the outbreak unfolds is the risk of saturating and actual saturation of critical points in the response system what are the critical points which will saturate and how does this effect ability to manage impact and reduce consequences. Politically paralyzed environments are particular vulnerable to falling behind the pace of the outbreak.
  
  Discussion with Tom Seager:
  
  **Resilience perspectives on pandemics**
  
  Looking at the Italian doctors interview about his ICU getting slammed and trying not to be overwhelmed by the surge.
  
  A lot of the stuff we talk about in general is evident in his experiences. It also parallels our study of a mass casualty response – the clinicians adapt to somehow make up a response that gets everyone through; the planning and plans contribute very little to the front lines.
  
  Analyses are using the #s (estimates based on current experience) to show what overload looks like. Note the critical # – length of stay in hospital until sufficient recovery – it’s a bigger than normal #. This is part of knowing how recovery processes happen after hospitalization. The Italian physician’s account of their adaptations to treat the growing need shows how front line clinical work adapts as the need they directly face grows. These are very local and an open question is how well are these processes/lessons being shared and set up by hospital systems not yet loaded with COVID-19 patients.
  
  The care issue shifts from a standard of care centered on each patient to a standard of care centered on handling the patient load — a set — while doing the best available for all patients (given the response resources mobilized) note how the Italian physician said this in effect. Simple example: in a standard of care centered on each patient, each patient gets a physician from the appropriate medical specialty while in a potential overload situation, physicians from other specialties are recruited in and work with the specialists – so that expertise is deployed to cover patient needs in new ways.
  
  We see a strange loop effect in this case, as well as in the IT infrastructure/DevOps incidents – providing care undermines the ability to provide care as the clinicians needed for care are themselves quarantined or get the disease, increasing cases, transmission risks, and reducing the ability to meet care needs, as is happening in northern Italy.
  
  The US is providing an extraordinary contrast: the incoherent non-system in US provides a variety of pathways for the virus to spread and as that happens the potential for overload is scary. Cross level coordination is the only way to generate new capacity ahead of need (this coordination also facilitates varies forms of horizontal sharing).
  
  There is no way to know what is sufficient effort to generate new readiness to respond. This means, if enough capacity is generated given the actual experienced load, afterwards there will be the perception of inefficiency or unnecessary or wasteful or overreaction – exactly what I wrote about the sacrifice judgement in the 2006 Essentials chapter, though with examples at a very different scale.
  
  Interview with WHO head on lessons from past outbreaks, Ebola.
  
  [https://7news.com.au/lifestyle/health-wellbeing/speed-trumps-perfection-boss-of-world-health-organisation-issues-blunt-coronavirus-warning-c-747300](https://7news.com.au/lifestyle/health-wellbeing/speed-trumps-perfection-boss-of-world-health-organisation-issues-blunt-coronavirus-warning-c-747300)
  
  Highlights how timing matters and need to act aggressively early — shows anticipation paradox. Also how the standard framing of decision making leaves out time; even he is caught in this framing, when he is supporting the fundamental reframing of naturalistic decision making – time (time pressure, tempo, keeping pace) always matters.
  
  The announced English government policy (already collapsing) that large gatherings do not need to be cancelled/banned and that schools do no need to be closed is hard to justify especially given NHS overload, presumably they wanted to try to reduce the economic losses of responding to the outbreak. Then they rolled out a justification claiming it was a scientifically based plan to build herd resistance to COVID-19 presumably by having the virus spread more widely among the groups who would have mild symptoms or recover without hospital interventions.
  
  I find this whole thing remarkably unethical. I can hardly begin to lay out how crazy this is on so many levels and dimensions. Has the anglo world lost its mind?
  
  Such a study requires approval by a human subjects review committee. Proposers of research do not get to decide themselves if their research plans meet safety criteria for people who participate in the planned research. I took the liberty of preparing the submission for Boris PM of UK:
  
  **Proposed research submitted for approval**
  
  **Method:**
  
  Let a novel virus spread through a country of 66 million people to measure the speed and scale of development of herd resistance.
  
  **Hypothesis:**
  
  (1) Immune system resistance to the new virus will develop sufficiently quickly across 60% or more of the population in the current outbreak to build herd resistance.
  
  (2) Immune resistance of greater than 60% of population will significantly reduce the virus’ mortality statistics by age group in some future outbreak of this virus.
  
  **What is a significant reduction?**
  
  Whatever reduction in illness/deaths that reduces fear in the population enough so that economic consequences are minimized.
  
  **Evidence basis for hypothesis?**
  
  Need to find a way to justify slow and inadequate responses to the current outbreak.
  
  **Consequences if hypothesis is supported:**
  
  Substantial mortality in age groups most at risk; collapse of NHS due to overload of patients requiring hospitalization. Reduce mortality and serious illness in a second wave of virus spreading through population in the future.
  
  **Consequences if hypothesis is incorrect:**
  
  Substantial mortality in age groups most at risk; collapse of NHS due to overload of patients requiring hospitalization.
  
  **Conclusion:**
  
  Obviously, the proposed research project is exempt from further review and can proceed.
  
  Social scientist (@RebeccaGruby) asks Ostrom’s Polycentric Governance should say a lot about how to handle outbreak.
  
  > 
  > 
  > 
  > The response to COVID-19 is a form of polycentric governance: multiple, overlapping decision-making centers operating with some autonomy, acting in ways that take account of others. Polycentricity scholars, what are some useful insights from our work? A few thoughts to start:
  > 
  > — Rebecca Gruby (@RebeccaGruby) [March 11, 2020](https://twitter.com/RebeccaGruby/status/1237853946217025537?ref_src=twsrc%5Etfw)
  > 
  
  She is correct and Resilience Engineering provides some results (from Resilience Engineering lectures on polycentric architectures):
  
  Polycentric governance intersects with nonlinear layered networks in resilience engineering (eg Doyle):
  
  1. how to be poised to adapt in advance of a future challenge?
  
  2. how to facilitate adaptive responses as a challenge unfolds?
  
  Ostrom’s Polycentric governance is a general finding: neither strictly hierarchical command architecture nor a completely decentralized flat network has sufficient adaptive capacity to overcome the basic risk of brittleness. Hard constraint for all systems & scales is “viability requires extensibility”. The discovery of graceful extensibility as a basic form of adaptive capacity required to overcome brittleness but it trades-off with pursuit of optimality. Thus, Ostrom really is posing a question: of the many architectures in between flat and command, what works well when & where? Those that build and sustain graceful extensibility. Another constraint comes from work on fundamental trade-offs: Need an architecture that builds in the ability to continue to adapt over longer cycles — to find new ways to balance constraints as change continues, especially since others adapt.
  
  Initial challenges of this outbreak operate at continental & inter-continental scale #1: Societies with experience at previous outbreaks (SARS) have anticipated, mobilized & generated responses better than those without — a difference in proactive learning.
  
  Once into a new challenge, what matters (#2)? Strategy in 4 parts
  
  A. Empower decentralized initiative at Sharp End Layers, up close roles. …
  
  B. Support reciprocity across roles as overload threatens by anticipating bottlenecks ahead.
  
  C. Broad End Layers, distant ‘supervisory’ roles coordinate/synchronize over emerging trends to meet changing priorities. …
  
  D. Horizontal learning & exchange will emerge spontaneously and informally across units/roles facing the challenges will emerge. Broad End Layers monitor for, facilitate, cross-check, & at least do not block this.
  
  See [Essentials of resilience, revisited](https://www.researchgate.net/publication/330116587_4_Essentials_of_resilience_revisited) & our studies on adaptation in a large mass casualty event at Researchgate.
  
  Ohio’s response demonstrates these moving forward.
  
  Preparing for another interview:
  
  List of possible points to discuss in interview:
- Steering through threats and obstacles: verb, not category
- Risk of saturation as patients and severity increases and threatens to overload response capabilities
- Deploy, mobilize, generate additional response capabilities – build readiness to respond in advance of patients needing hospitalization
- Steering through threats and obstacles at this scale means:
- Multi-party and multi-level steering that is synchronized to expand ability to keep pace with increasing demands
- At some point lets’ break up questions and responses: eg. Scale, anticipation – staying ahead of growth of need, overload issues, rationalizing away threat, not an episode but a novel event triggering a process that will go on over a longer time scale, natural experiment in US
- At continental scale and intercontinental scale: polycentric governance, vertical and horizontal coordination as learn ways to treat and expand and preserve response capabilities. How upper echelons generate and move resources to build effectiveness, entrain/help synchronize responses across scales, remove blockages, …
- Anticipation paradox: generate a readiness to respond in advance of the patient needs
- How this threat adds load and constraints on generating response capability
- Isolation units – reconfiguration
- Protective gear in “hot” zone
- How to handle arriving patients who might have COVID-19, given non-COVID-19 medical problems will arrive too
- The nature of the threat affects clinicians providing care, thus reducing the ability to provide the needed care:
- time in hot zone limited,
- how to extend effective time in hot zone,
- clinicians become potential vector for transmission,
- workers who need to be quarantined reduce clinical response capability,
- sustained performance issues
- 8% of cases in Italy are healthcare workers
- Not just an episode