1. Turn model into API server that can receive requests, run inference, and return response. ==> to standardize the model
2. Take API and containerize it.
3. Deploy image as a pod in k8s.

Deploy pod as a deployment
Service load balances replicas.

Kubeflow, BentoML+Yatai ==> Frameworks

