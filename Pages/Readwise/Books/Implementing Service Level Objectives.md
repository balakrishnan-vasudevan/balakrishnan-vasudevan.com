# Implementing Service Level Objectives

![rw-book-cover](https://m.media-amazon.com/images/I/91bW4csmp5L._SY160.jpg)

## Metadata
- Author: [[Alex Hidalgo]]
- Full Title: Implementing Service Level Objectives
- Category: #books

## Highlights
- Reliability is a conversation. It is a conversation we have with our infrastructure, our systems, and our services as we attempt to operate them. It is a conversation we have with complexity, security, scalability, and velocity in the hopes they will emerge in the way we need them. It is a conversation we have with ethics, privacy, and justice as we attempt to do the right thing for the people who depend on us. And finally, it is a conversation we have with our colleagues so we can work together to build what matters to us. ([Location 31](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=31))
- SLIs and SLOs help us think about, communicate, and interact with reliability in a new way. ([Location 38](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=38))
- First, a proper level of reliability is the most important operational requirement of a service. A service exists to perform reliably enough for its users, where reliability encompasses not only availability but also many other measures, such as quality, dependability, and responsiveness. The question “Is my service reliable?” is pretty much analogous to the question “Is my service doing what its users need it to do?” ([Location 221](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=221))
- Because of this, the second truth is that how you appear to be operating to your users is what determines whether you’re being reliable or not — not what things look like from your end. It doesn’t matter if you can point to zero errors in your logs, or perfect availability metrics, or incredible uptime; if your users don’t think you’re being reliable, you’re not. ([Location 227](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=227))
- the third thing that is always true is that nothing is perfect all the time, so your service doesn’t have to be either. Not only is it impossible to be perfect, but the costs in both financial and human resources as you creep ever closer to perfection scale at something much steeper than linear. Luckily, it turns out that software doesn’t have to be 100% perfect all the time, either. ([Location 234](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=234))
- An SLI is a measurement that is determined over a metric, or a piece of data representing some property of a service. A good SLI measures your service from the perspective of your users. It might represent something like if someone can load a web page quickly enough. People like websites that load quickly, but they also don’t need things to be instantaneous. An SLI is most often useful if it can result in a binary “good” or “bad” outcome for each event — that is, either “Yes, this service did what its users needed it to” or “No, this service did not do what its users needed it to.” For example, your research could determine that users are happy as long as web pages load within 2 seconds. Now you can say that any page load time that is equal to or less than 2 seconds is a “good” value, and any page load time that is greater than 2 seconds is a “bad” value. ([Location 246](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=246))
- At the next level of the stack we find service level objectives, or SLOs, which are informed by SLIs. You’ve seen how to convert the number of good events among total events to a percentage. Your SLO is a target for what that percentage should be. The SLO is the “proper level of reliability” targeted by the service. ([Location 257](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=257))
- An error budget is a way of measuring how your SLI has performed against your SLO over a period of time. It defines how unreliable your service is permitted to be within that period and serves as a signal of when you need to take corrective action. ([Location 273](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=273))
- SLAs are business decisions that are written into contracts with paying customers. You may also sometimes see them internally at companies that are large enough to have charge-back systems between organizations or teams. ([Location 285](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=285))
- A meaningful SLI is, at its most basic, just a metric that tells you how your service is operating from the perspective of your users. Although adequate starting points might include things like API availability or error rates, better SLIs measure things like whether it is possible for a user to authenticate against your service and retrieve data from it in a timely manner. ([Location 302](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=302))
- A good SLI is also able to be expressed meaningfully in a sentence that all stakeholders can understand. ([Location 306](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=306))
- And that’s all SLOs are. They’re targets for how often you can fail or otherwise not operate properly and still ensure that your users aren’t meaningfully upset. If a visitor to a website has a page that loads very slowly — or even not at all — once, they’ll probably shrug their shoulders and try to refresh. If this happens every single time they visit the site, however, the user will eventually abandon it and find another one that suits their needs. ([Location 323](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=323))
- It’s important to reiterate that SLOs are objectives — they are not in any way contractual agreements. You should feel free to change or update your targets as needed. Things in the world will change, and those changes may affect how your service operates. It’s also possible that those changes will alter the expectations of your users. Sometimes you’ll need to loosen your SLO because what was once a reasonably reachable target no longer is; other times you’ll need to tighten your target because the demands or needs of your users have evolved. ([Location 329](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=329))
- There are two very different approaches you can take in terms of calculating error budgets: events-based and time-based. ([Location 341](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=341))
- With the first approach, you think about good events and bad events. The aim is to figure out how many bad events you might be able to incur during a defined error budget time window without your user base becoming dissatisfied. The second approach focuses on the concept of “bad time intervals” (often referred to as “bad minutes,” even if your resolution of measurement is well below minutes.) This gives you yet another way of explaining the current status of your service. For example, let’s say you have a 30-day window, and your SLO says your target reliability is 99.9%. This means you can have 0.1% failures or bad events over those 30 days before you’ve exceeded your error budget. However, you can also frame this as “We can have 43 bad minutes every month and meet our target,” since 0.1% of 30 days is approximately 43 minutes. Either way, you’re saying the same thing; you’re just using different implementations of the same philosophy. ([Location 343](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=343))
- error budgets are just representations of how much a service can fail over some period of time. They allow you to say things like, “We have 30 minutes of error budget remaining this month” or “We can incur 5,000 more errors every day before we run out of error budget this month.” ([Location 351](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=351))
- For example, for a log processing pipeline you’d have to measure log ingestion rate and log indexing rate in order to get a good idea of the end-to-end latency of your pipeline; even better would be to measure how much time elapses after inserting a particular record into the pipeline before you can retrieve that exact record at the other end. Additionally, when thinking about SLIs for a pipeline, you should consider measuring the correctness of the data that reaches the other end. For lengthy pipelines or those that don’t see high volumes of requests per second, measuring data correctness and freshness may be much more interesting and useful than measuring end-to-end latency. Just knowing that the data at the end of the pipeline is being updated frequently enough and is the correct data may be all you need. ([Location 404](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=404))
- Batch jobs are series of processes that are started only when certain conditions are met. These conditions could include reaching a certain date or time, a queue filling up, or resources being available when they weren’t before. Some good things to measure for batch jobs include what percentage of the time jobs start correctly, how often they complete successfully, and if the resulting data produced is both fresh and correct. You can apply many of the same tactics we discussed for data processing pipelines to many batch jobs. ([Location 414](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=414))
- In terms of a database, the incoming data is likely very structured, the processing might be intense, and the response might be large. On the other hand, for storage system writes, the data may be less structured, the processing might mostly involve flipping bits on an underlying hardware layer, and the response might simply be a true or false telling you whether the operation completed successfully. The point is that for both of these service types, measuring things like latency and error rates — just like you might for a request and response API — can be very important. However, databases and storage systems also need to do other, more unique things. Like processing pipelines and batch jobs, they also need to provide correct and fresh data, for example. But, one thing that sets these service types apart is that you also need to be measuring data availability and durability. ([Location 421](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=421))
- Compute platforms are not overly difficult to measure, but they’re often overlooked when organizations adopt SLO-based approaches to reliability. The simple fact of the matter is that the services that run on top of these platforms will have a more difficult time picking their own SLO targets if the platforms themselves don’t have any. SLIs for services like these should measure things like how quickly a new VM can be provisioned or how often container pods are terminating. Availability is of the utmost importance for these service types, but you also shouldn’t discount measuring persistent disk performance, virtual network throughputs, or control plane response latencies. ([Location 434](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=434))
- Computing proper SLIs and setting appropriate SLOs generally requires a lot of data, so if you’re responsible for just a few racks in a data center somewhere, it might not make a lot of sense to expend a lot of effort trying to set reasonable targets. But if you have a large platform, you should be able to gather enough data to measure things such as hard drive or memory failure rates, general network performance, or even power availability and data center temperature targets. ([Location 443](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=443))
- SLOs guide you, they don’t demand anything from you. ([Location 463](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=463))
- Service level objectives are ultimately about happier users, happier engineers, happier product teams, and a happier business. This should always be the goal — not to reach new heights of the number of nines you can append to the end of your SLO target. ([Location 485](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=485))
- It’s a subset of systems engineering, which is the study of how to properly build complex systems (and has itself also been around for decades). ([Location 520](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=520))
- If others cannot find out how reliable you intend to be, they cannot voice if they’re happy or upset about this target. Additionally, if users that depend on your service cannot find out how reliable you intend to be, they cannot accurately set their own reliability targets. ([Location 574](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=574))
- your level of reliability in the past has likely become a sort of agreement with your users that they depend on, whether you intended this to happen or not. This is partly because your service likely has many more uses than you might originally realize. A great example of how to think about this is Hyrum’s law: With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody. Coined by Hyrum Wright, an engineer at Google, Hyrum’s law is a wise observation that is particularly important when you’re thinking about what to measure in terms of service reliability. Your users may expect certain outlier features to be reliable in ways that you never imagined. This all ties back to the implicit agreements you’ve made with the users of your service. You might not realize what your users actually need from you, so don’t be surprised if your reliability measurements need to change as you learn more about their behaviors. ([Location 582](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=582))
- What makes this service reliable to you, the user? What do you need, and what do you expect? What sorts of implicit or explicit agreements are there between this service and those who use it? The first things you might think about are things like, “When I start a movie, does it play?” This is a great starting point and absolutely a part of the story. A digital streaming service needs to be able to stream things, or it’s clearly not reliable. But the service doesn’t only need to be able to stream to your device. For example, it also needs to stream in a timely manner. A great example of not striving for perfection is the buffering time you might expect when you start a new streaming movie or TV show. Most people are going to be totally fine with several seconds of buffering time at the beginning of a viewing. The concept that the content needs to “load” is a thing users are used to, so it’s fine if starting things off takes a little while. It just can’t consistently take way too long, or people won’t consider the service reliable anymore and might move to a different service. On the other hand, you may not lose any users if it takes “too long” occasionally, as long as it doesn’t take that long every single time. While people are accustomed to buffering time at the beginning, they don’t want buffering to interrupt things in the middle of their viewing. So, while you can be reliable and buffer for a fair amount at one point in time, you might not be able to do so at other points in time and still be seen as reliable. However, there is much more to the reliability of a video stream than just starting in a reasonable amount of time and not stopping to buffer in the middle of the action. You also need the video stream to be the correct one. If you select the movie Anchorman, you’d better not get delivered an episode of Seinfeld. And this is true not only for how the stream starts — you also can’t have your selection switch right in the middle, even if the switch is otherwise seamless. Additionally, you need the stream to be of a certain quality. For most people a streaming service doesn’t have to be 100% perfect all the time, but other people specifically pay for guarantees of higher resolutions. So, in order to be reliable, the service needs to deliver good enough video quality for some people and very good video quality for others. Seeing the images on the screen is great, but most people will also need audio. And not only does the audio need to be present, but it needs to be synced to the video. If the audio lags behind the video by even a second or two the video might be unwatchable to some people. Additionally, the audio needs to be for the correct movie, in the correct language, and at an understandable quality and volume. Other users may not need audio at all, but in that case they’ll probably need for subtitles to be present, synced to the video, in the correct language, and in a font and size that are readable. When you search for a movie,… ([Location 595](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=595))
- Reliability is expensive in many different ways. The first and most obvious one is financially. To build systems that are able to tolerate failure, you need many things. You need your system to be distributed, so that a problem in one location doesn’t mean your entire service stops operating. It doesn’t matter if you run your service on your own hardware in your own data centers, via a cloud provider, or both. If you need your service to be highly available, you’re going to incur more expenses in pursuit of that goal, because you’ll need a presence in more than just one physical or logical location. Additionally, you’ll need your systems to have rigorous testing infrastructures. You’re much more likely to encounter failures and unreliability if you’re not properly vetting your changes before they go out. This could mean everything from QA and testing teams to staging environments and proper canarying techniques. All of these things are good, and should exist in any mature engineering organization, but they also cost money. The more comprehensive you want them to be, the more people and computing resources you’ll need. Beyond financial costs, there are also human ones. For example, let’s say you want to hit a common reliability target of 99.99% over a 30-day window. This target implies that you can only be unreliable for 4 minutes and 32 seconds during those 30 days.3 This further implies that anyone on call for this service needs to have a response time on the order of seconds. You can’t have a service that is 99.99% reliable if it takes your on-call engineers five minutes just to get in front of their laptops and start diagnosing things. People who are on call for a service that is aiming for 99.99% will need to always be within arm’s reach of a laptop, they’ll have to coordinate their commutes with their secondaries or backups, and they won’t be able to do the little things like go to a movie or take their dog on a walk during their shifts.4 ([Location 693](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=693))
- For example, when you think about going from 99.9% to 99.95% reliable, your intuition might tell you that it’s about the same increase as going from 99.95% to 99.99% reliable — but that’s not how the math actually works out. 99.9% reliable implies an unreliability of 0.1%, whereas 99.95% reliable implies an unreliability of 0.05%. Moving from 0.1% to 0.05% is a change factor of 2: Meanwhile, 99.99% reliable implies an unreliability of 0.01%. That means you’re moving from 0.05% to 0.01% if you’re going from 99.95% to 99.99%. This is a change factor of 5: Moving from 99.95% to 99.99% is thus 2.5 times more of a change than moving from 99.9% to 99.95%. The resources you’ll need to achieve this will need to increase proportionally, if not more so. This is why chasing a target that forever approaches, but never reaches, 100% will end up costing you more than you could plan for. ([Location 712](https://readwise.io/to_kindle?action=open&asin=B08FBP3ZRH&location=712))
