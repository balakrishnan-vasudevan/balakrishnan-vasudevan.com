# The Field Guide to Understanding 'Human Error'

![rw-book-cover](https://images-na.ssl-images-amazon.com/images/I/51o5UQmwcZL._SL200_.jpg)

## Metadata
- Author: [[Sidney Dekker]]
- Full Title: The Field Guide to Understanding 'Human Error'
- Category: #books

## Highlights
- ‘Human error’ requires a standard. For the attribution to make any sense at all, it requires the possibility of actions or assessments that are not, or would not have been, erroneous. ([Location 315](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=315))
- Putting in more rules, procedures and compliance demands runs into the problem that there is always a gap between how work is imagined (in rules or procedures) and how work is done. Pretending that this gap does not exist is like sticking your head in the sand. And trying to force the gap to close with more compliance demands and threats of sanctions will drive real practice from view. ([Location 460](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=460))
- says that what people do makes sense to them at the time—given their goals, attentional focus and knowledge—otherwise they wouldn’t be doing it. In other words: people do not come to work to do a bad job. Pilots do not check in for a flight in order to die. Nurses do not sign in to go kill a patient (and if they do, it takes you into the realm of sabotage, criminality, terrorism which requires different explanations and interventions—not part of this book). ([Location 516](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=516))
- The “tunnel.” Understanding ‘human error’ is about understanding the “inside” perspective—not the outside or hindsight one ([Location 548](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=548))
- psychotechnik, ([Location 604](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=604))
- Practitioners are not all exposed to the same kind and level of accident risk. This makes it impossible to compare their accident rates and say that some, because of personal characteristics, are more accident-prone than others. ([Location 621](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=621))
- they started asking what was responsible for safety and risk, not who. ([Location 643](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=643))
- Pilots are not simply entrusted by the system to stay competent and proficient on their own: the system actively helps them stay proficient and competent ([Location 662](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=662))
- A “Bad Apple” problem, to the extent that you can prove its existence, is a system problem and a system responsibility. ([Location 666](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=666))
- “error” is not a cause of trouble but a symptom of trouble. ([Location 668](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=668))
    - Tags: [[blue]] 
- Even the editors of the British Medical Journal have decided that a Bad Apple problem is a systems problem, and that addressing it is a systems responsibility: the time has come, they say, to design and evaluate systems that identify problematic individuals. So in healthcare, too, the recruitment and retaining of staff who turn out ineffective in their role is a systems issue—not one of defective personal accountability. ([Location 670](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=670))
- This accountability forms the other side of professional autonomy and competence, to be seen to be good at what you do, and accepting the consequences when things do not go well. Such accountability gives people considerable pride, and it can make even routine operational work deeply meaningful. ([Location 683](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=683))
- A systems approach understands that each component or contributor in a system has specific responsibilities to help attain the system’s overall goals. ([Location 732](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=732))
- There is no evidence that a system approach dilutes personal accountability. In fact, second victims show just how much responsibility practitioners take for things that go wrong. ([Location 745](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=745))
- As soon as you put people on the defensive, just imagine what happens to possibilities for learning from failure. They disappear. People will cover up, not tell you things, change or leave out inconvenient details. Accountability can mean letting people tell their account, their story. ([Location 749](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=749))
- The key is holding people accountable without invoking defense mechanisms. ([Location 751](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=751))
- If you hold somebody accountable, that does not have to mean exposing that person to liability or punishment.        • You can hold people accountable by letting them tell their story, literally “giving their account.”        • Storytelling is a powerful mechanism for others to learn vicariously from trouble. ([Location 762](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=762))
- A striking example of this occurred when, over a six-year period, ‘hundreds of mechanics were cited for logbook violations. People working the aircraft on the gate were under pressure and they’d screw up the paperwork.’ Violations meant suspensions or a fine. Then the airline wanted to print 50,000 new logbooks. Starting with the station that had most problems, it asked the mechanics to design the pages. They did. Another station made a few tweaks, and when the new logbooks were introduced, violations dropped to zero. The problem wasn’t negligent mechanics, it was a poorly designed logbook. ([Location 775](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=775))
    - Tags: [[orange]] 
- People with more power in the organization tend to see the culture as more “just.”        • Managers and supervisors can sometimes see a “just culture” program as a good way to get someone fired.        • There is little evidence that organizations which have implemented such schemes produce more learning. In fact, they sometimes encourage a climate of risk secrecy.        • There is also little evidence that practitioners avoid personal accountability, even when your organization has embraced a systems view. In fact, remember the findings on second victims mentioned above, which show just how much responsibility people take for things that go wrong on their watch.19        • Deciding whether behavior is an honest mistake or more culpable involves all kinds of value judgments (for example, about standards of care in the community, prudent persons, professional duties).20 Somebody will have to make those judgments. Who will that be in your organization? In other words, there is no pre-existing line between the categories. There are only people who draw it. Categories into which we put human and social features are infinitely negotiable. What is white to someone at some point, can easily become grey to someone else, or even black. ([Location 815](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=815))
- If you truly want to create accountability and a “just culture” in your organization, forget buying it off the shelf. It won’t work, independent of how much you pay for it. You need to realize that it is going to cost you in different ways than dollars. ([Location 831](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=831))
- 1. Don’t ask who is responsible, ask what is responsible.           Remember from the preface that, in the 1940s, human factors engineers and psychologists started asking what is responsible for errors, not who is responsible. Human factors showed that people’s actions and assessments make sense once we understand critical features of the world in which they work. People’s actions are systematically connected to features of their tools and tasks. Targeting those features (the what) is an action that contains all the potential for learning, change and improvement. Therefore, the first response to an incident or accident—by peers, managers and other stakeholders—should be to ask what is responsible, not who is responsible.       2. Link knowledge of the messy details with the creation of justice.           One of the more frustrating experiences by practitioners involved in an incident is that those who judge them often do not really know what their work is like. They do not know the messy details, they lack technical knowledge, misunderstand the subtleties of what it takes to get the job done despite the organization, the rules, the multiple constraints. Whether this is a supervisor, an inspector, the police, a judge, a jury—these are rarely “juries of peers.” These groups do not have the same intimate knowledge of the work they are judging, and they may also have incentives to build a story that puts the practitioner at a disadvantage. So make sure you have people involved in the aftermath of an incident who know the messy details, and who have credibility in the eyes of other practitioners.       3. Explore the potential for restorative justice.           Retributive justice focuses on the errors or violations of individuals. It suggests that if the error or violation (potentially) hurt someone, then the response should hurt as well. Others in the organization might have a desire to deny systemic causes, they might even fear being implicated in creating the conditions for the incident. Restorative justice, on the other hand, suggests that if the error or violation (potentially) hurt, then the response should heal. Restorative justice acknowledges the existence of multiple stories and points of view about how things could have gone wrong (and how they normally go right). Restorative justice takes the view that people do not come to work to do a bad job. Indeed, most people are willing to work constructively after a near miss has occurred. Restorative justice fosters dialogue between the actor and the surrounding community (for example, of colleagues), rather than a break in relationships through sanction and punishment.       4. Go from backward to forward-looking accountability.           Backward-looking accountability means blaming people for past events. The idea of “holding someone accountable” is used for events that have already happened. It implies some sort of sanction, removal or dismissal. It is not clear what people hope to achieve… ([Location 835](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=835))
    - Tags: [[orange]] 
- Of course, your organization cares about accountability. But it needs the kind of accountability that encourages learning. Accountability based on restorative justice can do that. It involves people telling their accounts, their stories. It involves expressing remorse for what happened and suggesting ways in which repetition might be prevented and relationships restored. Restorative justice does not nullify social obligations, it doesn’t get people off the hook. It is not about “cheap grace.” Instead, it sees accountability and learning as involving processes of disclosure, confession, apology, repentance and forgiveness. This also means that blame-free is not accountability-free. In fact, blame means less accountability: fewer accounts, less rich accounts. And less learning for your organization. If you let your reactions to failure, and superficial demands for “accountability” get in the way of understanding… ([Location 880](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=880))
    - Tags: [[orange]] 
- Reactions to failure are typically:        • Retrospective. They arise from your ability to look back on a sequence of events.        • Counterfactual. They lay out what people could or should have done to avoid the outcome that you now know about.        • Judgmental. They judge people for not doing what you believe they should have done, or for not paying enough attention to what you now know is important.        • Proximal. They focus on those people closest in time and place to (preventing) the mishap. ([Location 914](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=914))
- The more you react to failure, the less you will understand it. ([Location 925](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=925))
- The more you react, the less you understand. When you say “how could they have been so stupid to…!” or ask “how could they not have noticed…?” you are reacting to failure. These reactions block you from seeing how it could have made sense, and how it could make sense again to others you are responsible for. They block you from exploring the second story—the deeper, more complex organizational story behind a ‘human error’. ([Location 932](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=932))
    - Tags: [[orange]] 
- Hindsight means being able to look back, from the outside, on a sequence of events that led to an outcome you already know about.        • Hindsight gives you almost unlimited access to the true nature of the situation that surrounded people at the time (where they were versus where they thought they were; what state their system was in versus what they thought it was in).        • Hindsight allows you to pinpoint what people missed and shouldn’t have missed; what they didn’t do but should have done. ([Location 938](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=938))
- Hindsight causes you to oversimplify history, compared to how people understood events at the time they were happening:        • You think that a sequence of events inevitably led to an outcome. You underestimate the uncertainty people faced: you easily forget how unlikely the actual outcome seemed at the time. Had you seen their situation from the inside, you’d likely understand that the outcome (that you now know about) was once a small probability; one among many other possible outcomes.        • You see a sequence of events as linear, leading nicely and uninterruptedly to the outcome you now know about. Had you seen the same situation from the inside, you would have recognized multiple possible pathways and many zigs and zags in them..        • You oversimplify causality because you reason backwards. When you can trace a sequence of events backwards (which is the opposite from how people experienced it at the time), you easily couple “effects” to preceding “causes” (and only those causes) without realizing that causal couplings are much more difficult to sort out when in the middle of things. ([Location 945](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=945))
- Hindsight gets you to oversimplify history. You will see events as simpler, more linear, and more predictable than they once were. ([Location 957](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=957))
- there are actually two ways in which your understanding of a past situation gets influenced:        • The hindsight bias. Finding out about an outcome increases the estimate we make about its likelihood. In other words, as a retrospective reviewer who knows the outcome of an event, you exaggerate your own ability to predict and prevent the outcome—while not even being aware of that bias.2        • The outcome bias. Once you know the outcome, it changes your evaluation of decisions that led up to it. If the outcome is bad, then you are not only more willing to judge the decisions, but also more likely to judge them more harshly.3 ([Location 975](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=975))
- The perspective from the outside and hindsight (typically your perspective). From here you can oversee the entire sequence of events—the triggering conditions, its various twists and turns, the outcome, and the true nature of circumstances surrounding the route to trouble.        • The perspective from the inside of the tunnel. This is the point of view of people in the unfolding situation. To them, the outcome was not known (or they would have done something else). They contributed to the sequence of events because of what they saw on the inside of the unfolding situation. To understand ‘human error,’ you have to take this perspective. ([Location 993](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=993))
- When you say the following:         “they shouldn’t have…”         “they could have…”         “they didn’t…”         “they failed to…”         “if only they had…!” You are still reacting to failure. These are counterfactuals. They literally run “counter to the facts.” ([Location 1005](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1005))
- What you believe should have happened does not explain other people’s behavior. It just makes you look ignorant and arrogant. ([Location 1013](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1013))
- Counterfactuals say what could have happened if certain minute and often utopian conditions had been met. Counterfactual reasoning may thus be a fruitful exercise when recommending interventions against that exact failure in the future. But when it comes to explaining behavior, counterfactuals do not contribute. Counterfactuals are not opportunities missed by the people you are investigating. Counterfactuals are just the products of your hindsight. ([Location 1019](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1019))
- From the position of retrospective outsider, it is possible only to condemn people for turning a manageable situation into an irretrievable one. You will never be able to make sense of the behavior of those people. Real understanding comes from putting yourself in the shoes of the people on the inside of the sequence of events; on the inside of the tunnel. Only there can you begin to see why it made sense for people to do what they did. ([Location 1066](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1066))
- To understand error, take the view from the inside of the tunnel and stop saying what people failed to do or should have done. ([Location 1093](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1093))
- The blunt end gives the sharp end resources (for example equipment, training, colleagues) to accomplish what it needs to accomplish. But at the same time it puts on constraints and pressures (“don’t be late, don’t cost us any unnecessary money, keep the customers happy”). The blunt end shapes, creates, and sometimes even encourages opportunities for errors at the sharp end. ([Location 1138](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1138))
- Failures can only be understood by looking at the whole system in which they took place. But in our reactions to failure, we often focus on the sharp end, where people were closest to (potentially preventing) the mishap ([Location 1149](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1149))
- Really understanding safety and risk not only begins with calling off the hunt for Bad Apples or their errors. It means seeing both the blunt end and sharp end and how they interact to shape practice and determine what is normal or expected throughout an organization. This means zooming out, away from looking just at the sharp end, and incorporating blunt end policies and priorities and design choices and how these help drive people’s goals and practices at the sharp end. ([Location 1181](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1181))
- getting human factors data;        • building a timeline;        • putting data in context;        • leaving a trace;        • constructing causes;        • making recommendations. These steps will allow you to conduct an investigation that takes ‘human error’ out of the realm of soft data and mystery. You can gather hard data and build a set of traces that leave visible conclusions and useful recommendations. ([Location 1203](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1203))
- ‘Human error’ is not just about humans. It is about how features of people’s tools and tasks and working environment systematically influence human performance. So you need to gather data about all the features that are relevant to the event at hand. These might lie deeply buried in the organization surrounding people at the time—in its policies but also its unwritten preferences for on-time performance, for example. Let us look at two direct sources of human factors data that can help set you on a journey of discovery about these things:        • debriefings of participants;        • recordings of performance parameters. Debriefings of participants What seems like a good idea—ask the people involved in the mishap themselves—also carries a great potential for distortion. This is not because operators necessarily have a desire to bend the truth when asked about their contribution to failure. In fact, experience shows that participants are interested in finding out what went wrong and why. Rather, problems arise because of the inherent features of human memory:        • Human memory does not function like a videotape that can be rewound and played again.        • Human memory is a highly complex, interconnected network of impressions. It quickly becomes impossible to separate actual events and cues that were observed from later inputs.        • Human memory tends to order and structure events more than they were; it makes events and stories more linear and plausible. ([Location 1209](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1209))
- Klein proposes the following debriefing order:       1. Have participants tell the story from their point of view, without presenting them with any replays or reminders that supposedly “refresh their memory” but would actually distort it.       2. Tell the story back to them as investigator. This is to check whether you understand the story as the participants understood it.       3. If you had not done so already, identify (together with participants) the critical junctures in a sequence of events.       4. Progressively probe and rebuild how the world looked to people on the inside of the situation at each juncture. Here it is appropriate to show a re-play (if available) to fill the gaps that may still exist, or to show the difference between data that were available to people and data that were actually observed by them. At each juncture in the sequence of events (if that is how you want to structure this part of the accident story), you want to get to know:        • Which cues were observed (what did he or she notice/see or did not notice what he or she had expected to notice?)        • What knowledge was used to deal with the situation? Did participants have any experience with similar situations that was useful in dealing with this one?        • What expectations did participants have about how things were going to develop, and what options did they think… ([Location 1226](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1226))
- It is not uncommon that operators change their story, even if slightly, when they are debriefed on multiple occasions. Also, different participants who were caught up in the same events may come with a different take on things. How should you deal with this?        • Make the disagreements and inconsistencies, if any, explicit in your account of the event.        • If later statements from the same people contradict earlier ones, choose which version you want to rely on for your analysis and make explicit why.        • Most importantly, see disagreements and inconsistencies not as impairments of your investigation, but as additional human factors data for it. That people saw an unfolding situation differently can be crucial to your… ([Location 1246](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1246))
- Time is a powerful organizing principle, especially if you want to understand human activities in an event-driven domain. Event-driven means that the pace of activities is not (entirely) under control of the humans who operate the process. Things are happening in the process itself, or to it, that determine the tempo of, for example, people’s situation assessment and decision making. ([Location 1266](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1266))
- If you want to begin to understand ‘human error’ (for example, why people seem to have missed things, or decided things that, in hindsight, do not seem to make sense) a good starting point is to build a timeline. This timeline, however, needs to be of sufficient resolution to reveal underlying processes that may be responsible for the “errors.” In many attempts to understand ‘human error,’ timelines are of poor quality. Not because the data to build a good one weren’t available, but because people did not realize the importance of a sufficiently detailed timeline for gaining insight into human performance issues. ([Location 1271](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1271))
- While the kind of data you have access to can restrict the resolution of your analysis, that should not keep you from striving to get as much information out of your data trace as possible. For that, the sections below may still help. The second problem is the beginning of your timeline. Despite its strengths as an analytic tool, a timeline imports all the difficulties and limitations of the sequence-of-events model (you will learn more about this when you get to the chapter on accident models). What should the beginning of a sequence of events be? There is inherent difficulty in deciding what counts as the beginning (especially the beginning—the end of a sequence of events often speaks for itself). Beginning with the people who were closest in space and time to the eventual mishap is often the default choice. This may be fine, as long as you realize that the whole point of laying out their timeline is to understand why they did what they did. Beginning with what they did is only the first step for digging into the background that explains why. For that, you may have to go much higher or deeper, and much further back. Making clear where you start, and explaining this choice, is essential for a well-structured, credible ‘human error’ investigation. ([Location 1308](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1308))
- Basically, this timeline says what was said (and perhaps what was done). As to when it was said, this timeline provides a mere order of what came before what. A lot is lost when you represent your data that way. ([Location 1336](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1336))
- Conversation analysis uses an even finer-grained notation for systematically representing all kinds of aspects of talk (and non-talk activities). In comparison to the medium-resolution timeline, a timeline constructed using conversation analysis reveals even more about how things are said and done. It can make maximally visible how people themselves develop and understand their respective contributions to interaction and to the work required to operate their system. For example, a timeline using conversation analysis shows:        • The order of talk: how and when do participants switch roles as listener and speaker and how much of each role is played by the different participants.        • How much silence there is and when this happens.        • What overlap is there where different people talk simultaneously.        • Features of the manner of talk (stretching sounds, pitching up or down, slowing down talk or speeding it up, louder or softer talk).        • So-called tokens, such as “oh,” “um” and “ah.” These features are not just interesting in themselves. They all mean something, based on elaborate theories of social interaction. They can point to the nature of people’s interaction in managing a process, and how that interaction may have contributed to, or detracted from, managing an unfolding situation safely. ([Location 1366](https://readwise.io/to_kindle?action=open&asin=B00Q8XCSFI&location=1366))
