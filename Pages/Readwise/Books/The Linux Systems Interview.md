# The Linux Systems Interview

![rw-book-cover](https://m.media-amazon.com/images/I/71fLiRmnIGL._SY160.jpg)

## Metadata
- Author: [[Marker Kane]]
- Full Title: The Linux Systems Interview
- Category: #books

## Highlights
- How Interrupts Work   Interrupt Request (IRQ): When a hardware device needs to communicate with the CPU or the OS, it sends an IRQ to the CPU. The IRQ is usually sent through an interrupt line on the motherboard or the system bus.   Interrupt Handler: Once the CPU receives an IRQ, it stops the current process and transfers control to an Interrupt Handler routine in the OS. The Interrupt Handler routine is responsible for servicing the interrupt and determining what action needs to be taken.   Interrupt Service Routine (ISR): The Interrupt Handler routine typically calls an ISR to handle the interrupt request. The ISR is a specific routine that is designed to handle a particular type of interrupt. For example, a keyboard ISR might handle keyboard input, while a disk drive ISR might handle data transfer. NOTE: Interrupt Handlers and ISRs are often used interchangeably, but there is a subtle difference between them.  An Interrupt Handler is a piece of code that manages the overall interrupt handling process. It receives an interrupt request and performs some basic processing, such as acknowledging the interrupt, saving the context of the interrupted program, and dispatching control to the appropriate ISR.  An ISR, on the other hand, is a specific function responsible for handling a particular type of interrupt. The Interrupt Handler calls the appropriate ISR based on the type of interrupt received. The ISR then performs the specific processing required to handle that interrupt, such as reading data from a hardware device, updating data structures in memory, or sending data to an output device.   Interrupt Masking: The OS uses a mechanism called Interrupt Masking to prioritize Interrupt Requests as a way of preventing interrupts from interfering with critical OS tasks. When an interrupt occurs, the OS can choose to mask the interrupt and handle it later when it is more convenient.   Interrupt Nesting: This is a feature that allows multiple interrupts to occur at the same time. When an interrupt occurs while the CPU is already servicing another interrupt, the CPU saves the current state and starts servicing the new interrupt. Once the new interrupt is handled, the CPU returns to the previous interrupt and continues where it left off.     Interrupt Vector Table (IVT): This is a data structure in the OS that contains information about each interrupt request.… ([Location 199](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=199))
- Interrupt Masking: The OS uses a mechanism called Interrupt Masking to prioritize Interrupt Requests as a way of preventing interrupts from interfering with critical OS tasks. When an interrupt occurs, the OS can choose to mask the interrupt and handle it later when it is more convenient.   Interrupt Nesting: This is a feature that allows multiple interrupts to occur at the same time. When an interrupt occurs while the CPU is already servicing another interrupt, the CPU saves the current state and starts servicing the new interrupt. Once the new interrupt is handled, the CPU returns to the previous interrupt and continues where it left off. ([Location 217](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=217))
- Interrupt Vector Table (IVT): This is a data structure in the OS that contains information about each interrupt request. The IVT is used by the Interrupt Handler routine to determine which ISR to call when an interrupt occurs. ([Location 225](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=225))
- The flow of interrupts is as follows: The "Hardware Device" sends an IRQ to the CPU. The "CPU" receives the IRQ and transfers control to the "Interrupt Handler." The "Interrupt Handler" calls the appropriate "ISR" based on the interrupt type. The "ISR" performs specific processing related to the interrupt, such as reading data or updating data structures. Once the ISR completes its task, it returns control to the "Interrupt Handler." The "Interrupt Handler" then resumes the interrupted process, which could be a "Process" like a text editor.           The "Operating System" also plays a role in interrupt handling: ([Location 244](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=244))
- The "Operating System Interrupt Handler" manages the overall interrupt handling process in the OS. ●      The "IVT" is a data structure in the OS that contains information about each interrupt request and their corresponding ISRs. ●      The IVT is consulted by the "Interrupt Handler" to determine which ISR to call based on the interrupt type. ([Location 259](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=259))
- Chapter 2: System Calls     Before we dive into system calls, we establish a brief understanding around the privilege levels of the Linux OS. Access Rings   The Linux kernel has two main access rings: Ring 0 (also known as kernel mode) and Ring 3 (also known as user mode).  In Ring 0, the kernel and device drivers have the highest level of privilege and access to system resources. This includes direct access to hardware, memory management, and scheduling. The kernel operates in Ring 0 at all times and handles all system tasks. In Ring 3, user applications and processes run with limited access to system resources. In this mode, applications can only access system resources through system calls, which are mediated by the kernel. This provides a layer of protection between user applications and system resources, preventing malicious or buggy applications from causing harm to the system.       In this diagram, the ring numbers range from 0 to 3, with lower numbers representing higher levels of privilege and access to system resources. The highest privilege level is 0, which is reserved for the most sensitive system operations and is only accessible by the kernel and device drivers. The lowest privilege level is 3, which is used by user applications and processes and has the most limited access to system resources. System Calls   In Linux, system calls are a way for user-space programs to request services from the kernel. They provide a standardized interface between user-space and kernel-space, allowing programs to perform privileged operations such as creating or deleting files, allocating memory, and accessing hardware resources. How System Calls Work   A user-space program calls a function from a library, such as the C standard library. This function typically wraps a system call, providing a more convenient interface for the program to use.   The library function places arguments for the system call into registers or on the stack, depending on the calling convention for the corresponding architecture. These arguments specify the desired operation and any required parameters.   The library function executes a special instruction, such as the assembly instruction for x86 architecture CPUs INT 0x80, that generates a software interrupt and transfers control to the kernel. The processor responds by performing a context switch, where the processor switches… ([Location 279](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=279))
- The Linux kernel has two main access rings: Ring 0 (also known as kernel mode) and Ring 3 (also known as user mode).  In Ring 0, the kernel and device drivers have the highest level of privilege and access to system resources. This includes direct access to hardware, memory management, and scheduling. The kernel operates in Ring 0 at all times and handles all system tasks. In Ring 3, user applications and processes run with limited access to system resources. In this mode, applications can only access system resources through system calls, which are mediated by the kernel. This provides a layer of protection between user applications and system resources, preventing malicious or buggy applications from causing harm to the ([Location 284](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=284))
- system. ([Location 289](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=289))
- The Linux kernel uses a scheduler to manage the execution of processes. The scheduler determines which process to run next based on a set of scheduling policies and priorities. The scheduler also ensures that each process is given a fair share of CPU time and that high-priority processes are executed before low-priority ones.   Processes in Linux can communicate with each other using various IPC mechanisms, such as pipes, sockets, shared memory, and signals. This allows processes to exchange data and synchronize. ([Location 527](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=527))
- Process State: ([Location 535](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=535))
- The process state represents the current state of the process. It can be one of several states, such as running, sleeping, waiting, or stopped.   Memory Layout: The memory layout describes the virtual address space of the process. It includes areas such as the text segment (containing the executable code), the data segment (containing initialized data), the BSS segment (containing uninitialized data), and the heap and stack segments.   File Descriptors: The file descriptors represent the files or other resources that the process has open. Each file descriptor has an associated file table entry that describes the file's state and location.   Scheduling Information: The scheduling information includes the process's priority, scheduling policy, and other parameters that affect how the process is scheduled.   Signal Handlers: Signal handlers are functions executed in response to signals received by the process. The task_struct structure contains a list of signal handlers that the process has registered.     Parent and Child Process IDs: Each process has a Parent Process ID (PPID) that identifies its parent process, as well as one or more child PID that identify its child processes. ([Location 536](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=536))
- Text segment: This segment contains the executable code of the process. It is read-only and shared among all processes that use the same executable code.  ●      Data segment (initialized): This segment contains the global and static variables used by the process. It is typically read-write and initialized at program startup.  ●      BSS segment (uninitialized): This segment contains the uninitialized data for the process e.g., uninitialized global and static variables. It is typically read-write and initialized to zero at program startup.  ●      Heap segment: This segment is used for dynamic memory allocation. It is typically read-write and grows upward in memory as the process requests more memory.  ●      Stack segment: This segment is used to store local variables and function calls during program execution. It is typically read-write and grows downward in memory as the process pushes more data onto the stack.  ●      Memory-mapped files segment: This segment is used for memory-mapped files, which allows files to be accessed as if they were part of the process's memory. It is typically read-write and can be shared among multiple processes. ([Location 559](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=559))
- This involves using a system call like fork() or clone() to create a new process. The fork() system call creates a copy of the current process, while the clone() system call creates a new process with shared resources.   Executing the Program: After a process has been created, the next stage is to execute the program. This involves using a system call like exec() to replace the current process's memory space with a new program. The new program is loaded into memory and its main() function is executed. The parent process calls the wait() system call to receive the exit status of the child process and clean up the child resources.   Process Termination: The final stage in the lifecycle of a process is process termination. This involves using a system call like exit() to terminate the current process. The exit() system call performs various cleanup tasks, including closing open files and releasing memory.   Process Suspension and Resumption: During its lifetime, a process may be suspended and resumed by the kernel. System calls that manage the process's state include  wait(), waitpid(), and kill(). The wait() system call suspends the current process until a child process terminates, while the kill() system call sends a signal to a process to request that it is suspended or terminated.   Interprocess Communication (IPC): Processes can communicate with each other using system calls like pipe(), socket(), and mmap(). These system calls provide various mechanisms for processes to exchange data and synchronize. ([Location 582](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=582))
- A zombie process is a process that has completed execution but still has an entry in the process table. This happens when a parent process fails to call the wait() system call to collect the exit status of its terminated child process. As a result, the terminated child process becomes a zombie process, and its resources are not fully released until the parent process collects its exit status by calling wait() or waitpid(). Zombie processes do not consume any CPU time, but they do consume system memory and can eventually cause the system to run out of process table entries. To prevent this, it is important for parent processes to properly manage their child processes by calling wait() or waitpid(). ([Location 603](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=603))
- An orphan process is a process that is still running but has lost its parent process. This happens when the parent process terminates before the child process. Subsequently, the child process becomes an orphan process and is inherited by the init process (PID 1), which becomes its new parent. The init process periodically checks for orphan processes and collects their exit status by calling wait() or waitpid(). ([Location 625](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=625))
- Chapter 4: Memory Management     When a process is created in Linux, it is given its own virtual address space, which is a range of memory addresses that the process can use. This virtual address space is divided into small chunks called pages, which are typically 4KB in size. Physical RAM is also divided up into small chunks of the same size called page frames. Each page frame is given a unique virtual address, and the process can access the contents of that page frame by using that address.   The virtual addresses used by the process are not the same as the physical addresses used by the computer's memory hardware. Instead, the Linux kernel uses a page table to map virtual addresses to physical addresses. Page tables are data structures that contain information about the virtual-to-physical address mappings for each page in the process's address space.   When the process tries to access the contents of a page, the CPU generates a virtual memory address. The Linux kernel uses the page table to look up the physical address corresponding to that virtual address, and then the CPU can access the contents of the page at the physical address.   The page table is stored in memory, and it can become quite large for processes that use a lot of memory. To optimize performance, the Linux kernel uses a technique called "paging" to move pages between memory and disk. Pages that are not currently being used by the process can be moved to disk, freeing up physical memory for other processes to use. When the process needs to access a page that is not currently in memory, the Linux kernel then loads it back into physical memory from disk.   To keep track of which pages have been modified since they were last loaded from disk, the Linux kernel uses a "dirty bit" for each page. When the process modifies the contents of a page, the dirty bit is set to indicate that the page has been changed. When the Linux kernel needs to move a page from memory to disk, it checks the dirty bit to see if the page needs to be written back to disk first. Memory Management Unit (MMU)   The MMU is a hardware component in the CPU that plays a crucial role in the memory management of a process in the Linux kernel. The MMU translates virtual addresses used by the process into physical addresses used by the hardware. When a process attempts to access a page that is not currently in physical memory, the MMU generates a "page fault" exception. A page fault is an interrupt (see chapter 1 regarding interrupts) raised by the MMU to signal to the Linux kernel that the process is attempting to access a page that is not currently in physical memory. At this point, the kernel must handle the page fault by bringing the missing page into physical memory and updating the page table to reflect the new mapping. The Linux kernel handles page faults using the page fault handler, a section of code that is executed when a page fault occurs. This determines the cause of the page fault and takes the… ([Location 661](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=661))
- Chapter 5: Memory Allocation     In this chapter, we’ll discuss how a process allocates memory dynamically. This is often required by a program, e.g., a user keeps inputting values to the program, and a linked list needs to keep adding nodes for each input value. To dynamically allocate memory, a program can increase the size of the heap or the stack.   A quick review of the layout of the memory segments of a process:   In this diagram, the argv and environ pointers are located at the top of the address space, followed by the stack segment, which grows downwards toward lower memory addresses. The unallocated memory section represents the space that is not currently being used by the process. The heap segment is located below the unallocated memory section and grows upwards toward higher memory addresses. The BSS segment, which contains uninitialized global and static variables, is located below the heap, and the data segment, which contains initialized global and static variables, is located below the BSS segment. Finally, the text segment, which contains the program's executable code, is located at the bottom of the address space. Using the Heap   brk() and sbrk() system calls   The heap is a region of memory in a process's virtual address space that is used for dynamic memory allocation. In Linux, the heap is managed by the kernel using two system calls: brk() and sbrk().    When a program starts up, the kernel sets the program break, which is the end of the process's data segment, to a fixed address in memory. This address marks the initial end of the heap, and all memory beyond it is unallocated. The brk() system call can be used to set the program break to a new value, effectively resizing the heap. The new value must be greater than or equal to the current program break and less than or equal to the maximum size of the process's address space. The sbrk() system call is used to increment the program break by a specified amount, effectively allocating additional memory on the heap. The value passed to sbrk() specifies the number of bytes to add to the heap, and the return value is a pointer to the start of the newly allocated memory.   Malloc and free   In addition to brk() and sbrk(), the C standard library provides a memory allocation function called malloc(). In C, this function is used to dynamically allocate memory on the heap, while the free() function is used to deallocate memory that was previously allocated by malloc().    When a program calls malloc(), the C library implementation of this function keeps track of the size and location of the allocated memory block. This information is typically stored in a data structure called the heap metadata, which is also stored on the heap. When the program is finished using the allocated memory block, it should call free() to release the memory back to the heap. When free() is called, it marks the memory block as free in the heap metadata and adds it to a free list for later reuse. If… ([Location 721](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=721))
- Chapter 6: Signals     In the Linux kernel, signals are a mechanism for IPC. Signals allow a process to send a notification to another process or to itself. When a process receives a signal, it can perform a specific action, such as terminating or interrupting a system call.   Here are some important definitions/details about signals in the Linux kernel:   Signal numbers: Signals are identified by numbers which range from 1 to 64. Each signal has a specific meaning and can trigger a particular action. For example, signal 1 (SIGHUP) is typically used to request a process to reload its configuration file.   Signal handlers: When a process receives a signal, it can execute a user-defined signal handler function to handle the signal. A signal handler is a function that is registered with the kernel and executed by the process when it receives the signal. The default action for most signals is to terminate the process, but a signal handler can override this behavior.   Signal delivery: When a process receives a signal, the kernel suspends the process's execution and delivers the signal to the process. The process can then either handle the signal or defer its handling by blocking the signal until a later time.   Signal masking: A process can mask certain signals to prevent them from being delivered. When a signal is masked, the kernel will not deliver it to the process until it is unmasked again. This allows a process to avoid being interrupted by certain signals during critical operations.   Real-time signals: These are a special type of signal that allows a process to specify a data payload to be delivered along with the signal. Real-time signals are identified by numbers from 32 to 64 and can be used for high-priority communication between processes. Common Signals   Some of the most common signals in the Linux kernel and their default action are:   SIGTERM (15): This signal is sent to a process to request it to terminate gracefully. The default action is to terminate the process.   SIGKILL (9): This signal is used to force a process to terminate immediately. The default action is to terminate the process.   SIGINT (2): This signal is sent to a process when the user types Ctrl-C in the terminal. The default action is to terminate the process.   SIGHUP (1): This signal is used to request a process when its controlling terminal is closed. The default action is to terminate the process.   SIGUSR1 (10) and SIGUSR2 (12): These signals are user-defined and can be used for any purpose. The default action is to terminate the process, but they are often handled by user-defined signal handlers.   SIGSTOP (19) and SIGTSTP (20): These signals are used to suspend a process temporarily. The default action is to stop the process.   SIGCONT (18): This signal is used to resume a stopped or suspended process. The default action is to continue the process.   These are just a few examples of the many signals available in the Linux kernel. Each signal has a specific… ([Location 812](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=812))
- Chapter 7: Scheduling     Linux Scheduler   In Linux, process scheduling is handled by the kernel's scheduler. This is responsible for deciding which process should be executed on the CPU at any given time. The scheduler works by using timer interrupts generated by a hardware timer at a fixed frequency to interrupt the currently executing process and allow the scheduler to make a decision about which process should run next.    When a timer interrupt occurs, the scheduler's interrupt handler is executed. This is responsible for checking whether the currently running process has exceeded its time slice, which is a fixed amount of time that the process is allowed to run before it is preempted. If the process has exceeded its time slice, the interrupt handler preempts the process and selects a new process to run.    The scheduler uses a variety of algorithms to decide which process should run next. The most commonly used algorithm is the Completely Fair Scheduler (CFS), which is based on the idea of fair sharing of the CPU among all running processes. Under the CFS algorithm, each process is assigned a "virtual runtime" based on its priority and the amount of CPU time it has already consumed. The scheduler selects the process with the smallest virtual runtime to run next, which ensures that all processes are given a fair share of the CPU.    Other scheduling algorithms, such as the Round Robin Scheduler, are also available in Linux and can be selected based on the specific needs of the system.    In addition to timer interrupts, the scheduler is also triggered by other events, such as I/O completion or process wakeups. For example, when a process is blocked waiting for I/O, it is temporarily removed from the run queue and placed on a separate queue for blocked processes. When the I/O operation completes, the process is placed back on the run queue, and the scheduler may select it to run again.    Process scheduling in Linux is a complex and dynamic process that is critical to the efficient operation of the system. By using timer interrupts and a variety of scheduling algorithms, the Linux scheduler is able to ensure that all processes are given a fair share of the CPU and that system resources are used efficiently.   Process States   The diagram represents the different states a process can be in, using color-coded nodes and arrows to illustrate the transitions between states. ●      The blue node labeled "PROCESS" represents the general process entity. ●      The green node labeled "Running" represents the state where the process is actively executing on the CPU. ●      The orange node labeled "InterruptibleSleep" represents the state where the process is waiting for a resource that is not immediately available but can be awakened by a signal. ●      The light orange node labeled "UninterruptibleSleep" represents the state where the process is waiting for a resource that is not immediately available and cannot be interrupted by signals. ●      The… ([Location 961](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=961))
- Chapter 8: Linux I/O Model     Overview   The Universal I/O Model is a key component of the Linux kernel, which provides a standardized and efficient way of handling input and output operations across different types of devices. The main goal of the Universal I/O Model is to abstract the underlying hardware details of the various devices and provide a uniform interface for application programs to interact with them.   In the Linux kernel, the Universal I/O Model is implemented using a layered approach, where the various layers communicate with each other to perform the input and output operations. The following are the main layers involved in the Universal I/O Model: ●      User space: This layer contains the application programs that use the input/output operations. The application programs use the standard input/output functions provided by the C library, such as fread() and fwrite(), to communicate with the universal I/O subsystem. ●      Virtual file system (VFS): This layer provides a uniform interface to the various file systems supported by the Linux kernel. The VFS layer intercepts the input/output operations initiated by the application programs and routes them to the appropriate file system. ●      File system: This layer handles the input/output operations for a specific file system, such as ext4 or NTFS. The file system layer communicates with the appropriate device driver to perform the input/output operations on the physical device. ●      Device driver: This layer provides the interface between the kernel and the physical device. The device driver translates the input/output requests into commands that are understood by the hardware. ●      Hardware: This layer represents the physical device that is being accessed. ●      When an application program initiates an input/output operation, the Universal I/O Model follows this layered approach to perform the operation. The VFS layer intercepts the operation and identifies the appropriate file system, which in turn communicates with the device driver. The device driver then sends the appropriate commands to the hardware to perform the operation. Advantages   The Universal I/O Model provides several advantages, including:   Uniform interface: The model provides a uniform interface for application programs to interact with different types of devices. This simplifies the development of applications that need to access multiple devices.   Efficient performance: The model provides efficient performance by minimizing the number of context switches between the kernel and user space. This reduces overhead and improves the overall system performance.   Device independence: The model abstracts the hardware details of the devices, which allows applications to be developed without any dependencies on specific devices. This makes it easier to port applications across different platforms.   Overall, the Universal I/O Model is a key component of the Linux kernel that provides a standardized and… ([Location 1093](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=1093))
- Chapter 9: Linux File System Implementation     Overview   A Linux file system is a method of organizing and storing files on a computer's storage devices, such as hard drives or solid-state drives. It provides a hierarchical structure for organizing files and directories, along with mechanisms for file access, permissions, and metadata storage.   The Virtual File System (VFS) is an abstraction layer in the Linux kernel that provides a unified interface for different file systems. It allows applications and system components to access files and directories, while abstracting away from the underlying file system implementation. The VFS also provides a set of generic operations that file systems must implement to enable compatibility and interoperability.   File systems are implemented through a combination of software and data structures. The implementation typically involves designing data structures to represent files, directories, metadata, and file system operations. These data structures are managed by software routines that handle file system operations, such as creating, reading, writing, and deleting files. Ext2   One implementation of a Linux file system is the ext2 (second extended file system). The ext2 file system was widely used in earlier versions of Linux and consists of several components, including:   Superblock: Contains metadata about the file system, such as its size, block size, and the location of other important structures.   Inode table: Stores metadata about individual files, such as permissions, timestamps, and file size. Each file in the ext2 file system has an associated inode entry.   Data blocks: These are used to store the actual file contents. Data blocks are organized into block groups to improve performance and manage fragmentation.   Directory structure: Directories are special files that store references to other files and directories. They use a hierarchical structure, with each directory entry containing the filename and the corresponding inode number.   When a file system operation is performed on ext2, the VFS layer receives the request and forwards it to the appropriate ext2 file system driver. The driver then interacts with the ext2-specific data structures to execute the operation. For example, creating a file involves allocating a new inode entry and data blocks, and updating the directory structure and the superblock to reflect the changes. Ext2 Data Blocks   In the ext2 file system, data blocks are used to store the actual contents of files. To efficiently manage the storage of file data, it employs a combination of direct, indirect, and doubly indirect pointers.   Direct Pointers: The first twelve entries in the inode structure of ext2 are direct pointers. Each one of these points to a specific data block that contains a portion of the file's content, providing fast access to the most frequently accessed blocks of the file.   Indirect Pointers: The thirteenth entry in the inode structure is a single… ([Location 1348](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=1348))
- file is a directory, the inode will also contain a list of directory entries, allowing the file system to recursively resolve paths within the directory hierarchy. Hard and Soft Links   In a Linux file system, a hard link is a reference to a file or directory that shares the same underlying inode as the original file or directory. This means that a hard link points directly to the inode of the original file, and any changes made to the original file are immediately visible through the hard link.   When you create a hard link using the ln command, you essentially create a new directory entry that points to the same inode as the original file. Both the original file and the hard link have the same file permissions, ownership, and other metadata since they share the same inode.   Hard links are beneficial because they allow you to create multiple names for the same file or directory without creating additional copies of the data on disk. This can save disk space and reduce the time needed to create or copy files since only a new directory entry needs to be created to create a hard link.   However, hard links have some limitations. For example, they cannot be created for directories that reside on different file systems and can cause confusion if multiple hard links with different names are created for the same file.   In contrast to hard links, which point directly to the inode of the original file, soft links (also known as symbolic links or symlinks) are special files that point to the file name or path of the original file.   When you create a soft link using the ln -s command, you create a new file that contains the name or path of the original file. When a process accesses the soft link, the file system looks up the target file or directory based on the path stored in the soft link and provides the data to the process.   Soft links are beneficial because they allow you to create multiple names for the same file or directory, even if they reside on different file systems or partitions. Soft links can also be used to create shortcuts or aliases to files or directories that are buried deep in the file system hierarchy.   However, soft links have some limitations. For example, if the original file or directory is moved or renamed, the soft link may break since it points to the original file or directory by name. Soft links can also cause confusion if multiple soft links with different names are created for the same file or directory since they may appear to be separate files or directories to the user. Inode Table in Action   When you run the ls -l command in a Linux terminal, it lists the contents of a directory and displays detailed information about each file and subdirectory, such as the file permissions, owner, group, size, and modification time.   To obtain this information, ls -l reads the directory's inode table, which contains a list of inodes for all the files and directories in the directory. For each inode, ls -l reads the associated… ([Location 1432](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=1432))
- Sockets in the Linux kernel are a fundamental mechanism for network communication. They provide an interface for processes to send and receive data over the network, facilitating communication between different machines or processes running on the same machine. ([Location 1549](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=1549))
- Socket API: The Linux kernel exposes a socket API that allows user-space applications to create, configure, and interact with sockets by providing a set of system calls that applications can use to perform various socket operations, such as socket creation, binding, listening, connecting, sending, and receiving data.   Socket Data Structures: In the Linux kernel, sockets are represented by data structures. The main data structure is “struct socket”, which contains information about the socket's type, state, protocol, file operations, and other attributes. Each socket is associated with a file descriptor that user-space processes can use to access the socket.   Socket Types: The Linux kernel supports different socket types, such as TCP/IP sockets, UDP sockets, raw sockets, Unix domain sockets. Each socket type has its own characteristics and usage patterns. For example, TCP/IP sockets provide reliable, connection-oriented communication, while UDP sockets offer unreliable, connectionless communication. ([Location 1552](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=1552))
- Protocol Handlers: The Linux kernel implements various protocol handlers to handle different socket types and protocols. These handlers, often implemented as kernel modules, provide the necessary functionality to process and manage socket-specific operations. For instance, the TCP protocol handler handles TCP/IP sockets, while the UDP protocol handler handles UDP sockets.   Network Stack: Sockets are an integral part of the Linux networking stack. The networking stack is responsible for managing network connections, routing, packet transmission, and other networking tasks. Sockets interact with the networking stack to send and receive data packets, perform protocol-specific operations, and handle network-related events.   Socket Buffer: When data is sent or received through a socket, it is stored in a socket buffer, which is a kernel data structure that holds the data until it is processed. The socket buffer is managed by the kernel and is associated with each socket. Socket buffers are used to handle data transmission, buffering, and flow control within the kernel.   Socket Options: Sockets in the Linux kernel support various socket options that allow applications to configure the socket behavior. Socket options can control features such as socket timeout, buffer sizes, socket-level multicast settings, and more. These are typically set using the setsockopt() system call. Overall, sockets in the Linux kernel provide a powerful and flexible mechanism for network communication. They allow processes to establish connections, exchange data, and interact with the underlying networking infrastructure, enabling a wide range of networking applications and protocols. System Calls   To interact with sockets in Linux, various system calls are provided, such as:   socket() - Creates a new socket and returns a file descriptor that can be used to access the socket.   bind() - Associates a socket with a specific address, such as an IP address and port number.   listen() - Prepares a socket to accept incoming connections.   accept() - Waits for a connection request to arrive and returns a new socket for communicating with the client.   connect() - Establishes a connection to a remote socket.   send() - Sends data over a socket.   recv() - Receives data from a socket.   close() - Closes a socket and releases its resources.   These system calls can be used by… ([Location 1564](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=1564))
- Power-on or Reset: The computer is turned on or restarted, and the basic hardware initialization begins. ●      BIOS/UEFI: The Basic Input/Output System (BIOS) or Unified Extensible Firmware Interface (UEFI) firmware initializes and performs a Power-On Self-Test (POST) to check the hardware components. ●      Boot Loader: The boot loader (such as GRUB or LILO) is loaded and executed from the boot sector of the hard disk or some other bootable device. It presents a menu of available kernels and options, then loads the selected kernel into memory. ●      Kernel: The Linux kernel is loaded into memory and initialized. It detects and configures the hardware devices, mounts the root file system, and starts the init process. ●      Init System: The init system (such as systemd or SysVinit) is responsible for starting and stopping system services and daemons, and running scripts during the boot process. It also sets up the user environment and launches the user session. ●      User Session: The user session starts, typically with a login screen or graphical desktop environment, and the user can begin using the system. ([Location 1658](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=1658))
- Question: You get a ticket that a bad file keeps growing and filling up disk space. How would you approach this?   Possible answer:   Here's a systematic approach to address the issue:   Identify the file: Begin by locating the specific file that is consuming disk space. This can be done using various methods, such as examining disk usage reports (df or du) or using file exploration tools (ls, find, etc.).   Use lsof to identify the process: Once you have identified the file, you can use the lsof command with the file name as an argument to determine which process has the file open. Running lsof /path/to/file will provide information about the process associated with the file, including its PID.       Perform strace on the process: Utilize the strace command with the PID from the previous step to trace the system calls made by the process. Running strace -p <PID> attaches strace to the process and displays detailed information about its activities, including file operations.   By examining the strace output, you can identify the specific file-related system calls made by the process, such as open, write, or truncate. This can help pinpoint any erroneous behavior or abnormal file operations.   Investigate the application logs: Check the logs of the application associated with the problematic process. Application logs might provide insights into any error messages, warnings, or unusual behavior that could explain the file growth.   Check for configuration issues: Examine the configuration settings of the application or process involved. Misconfigured settings, such as log rotation or backup mechanisms, could contribute to the file’s continuous growth.   Consider other system monitoring tools: Utilize additional system monitoring tools like iotop or sysstat to gather information about disk I/O, CPU, and memory usage. These tools can help identify abnormal resource utilization patterns that may contribute to the file growth.   Implement a resolution: Once the root cause has been identified, you can proceed with the appropriate resolution. This might involve fixing the application configuration, modifying the logging settings, addressing code issues, or implementing file size limits.   This is a general troubleshooting approach, and the specific steps may vary depending on the system, file type, and context of the issue at hand.   Question: You are faced with an alarm indicating high p90 API latency coming from a particular host. How would you approach this?   Possible Answer:   Check system resource utilization: Start by checking the overall system resource utilization to identify any bottlenecks. The key areas to investigate include CPU, memory, disk I/O wait time, and network utilization. ●      To check CPU utilization: Use tools like top, htop, or sar to monitor CPU usage, identify any high CPU load, and determine if it's causing the high latency. ●      To check memory utilization: Use commands like free or tools like top to analyze memory… ([Location 2289](https://readwise.io/to_kindle?action=open&asin=B0DKSGPDY6&location=2289))
