# DevOps Troubleshooting

![rw-book-cover](https://images-na.ssl-images-amazon.com/images/I/51RdMrou2NL._SL200_.jpg)

## Metadata
- Author: [[Kyle Rankin]]
- Full Title: DevOps Troubleshooting
- Category: #books

## Highlights
- A system load average is equal to the average number of processes in a runnable or uninterruptible state. Runnable processes are either currently using the CPU or waiting to do so, and uninterruptible processes are waiting for I/O. ([Location 505](https://readwise.io/to_kindle?action=open&asin=B00A4G7M06&location=505))
- What is important to determine is whether the load is CPU-bound (processes waiting on CPU resources), RAM-bound (specifically, high RAM usage that has moved into swap), or I/O-bound (processes fighting for disk or network I/O). ([Location 525](https://readwise.io/to_kindle?action=open&asin=B00A4G7M06&location=525))
    - Tags: [[blue]] 
- Typically systems seem to be more responsive when under CPU-bound load than when under I/O-bound load. I’ve seen systems with loads in the hundreds that were CPU-bound, and I could still run diagnostic tools on those systems with pretty good response times. On the other hand, I’ve seen systems with relatively low I/O-bound loads on which just logging in took a minute because the disk I/O was completely saturated. A system that runs out of RAM resources often appears to have I/O-bound load, since once the system starts using swap storage on the disk, it can consume disk resources and cause a downward spiral as processes slow to a halt. ([Location 530](https://readwise.io/to_kindle?action=open&asin=B00A4G7M06&location=530))
- When you diagnose a slow system, one of the first values you should look at is I/O wait so you can rule out disk I/O. If I/O wait is low, then you can look at the idle percentage. If I/O wait is high, then the next step is to diagnose what is causing high disk I/O, which I will cover momentarily. If I/O wait and idle times are low, then you will likely see a high user time percentage, so you must diagnose what is causing high user time. If the I/O wait is low and the idle percentage is high, you then know any sluggishness is not because of CPU resources, and you will have to start troubleshooting elsewhere. This might mean looking for network problems, or in the case of a web server, looking at slow queries to MySQL, for instance. ([Location 608](https://readwise.io/to_kindle?action=open&asin=B00A4G7M06&location=608))
- If you see high user CPU time but low I/O wait times, you simply need to identify which processes on the system are consuming the most CPU. By ([Location 616](https://readwise.io/to_kindle?action=open&asin=B00A4G7M06&location=616))
    - Tags: [[orange]] 
- The most common high-CPU-load situations you will see are all of the CPUs being consumed either by one or two processes or by a large number of processes. Either case is easy to identify since in the first case the top process or two will have a very high percentage of CPU and the rest will be relatively low. In that case, to solve the issue you could simply kill the process that is using the CPU (hit K and then type in the PID number for the process). In the case of multiple processes, you might have one system doing too many things. You might, for instance, have a large number of Apache processes running on a web server along with some log parsing scripts that run from cron. All of these processes might be consuming more or less the same amount of CPU. The solution to problems like this can be trickier for the long term. As in the web server example, you do need all of those Apache processes to run, yet you might need the log parsing programs as well. In the short term, you can kill (or possibly postpone) some processes until the load comes down, but in the long term, you might need to consider increasing the resources on the machine or splitting some of the functions across more than one server. ([Location 629](https://readwise.io/to_kindle?action=open&asin=B00A4G7M06&location=629))
- The Linux kernel also has an out-of-memory (OOM) killer that can kick in if the system runs dangerously low on RAM. When a system is almost out of RAM, the OOM killer will start killing processes. In some cases this might be the process that is consuming all of the RAM, but this isn’t guaranteed. It’s possible the OOM killer could end up killing programs like sshd or other processes instead of the real culprit. In many cases, the system is unstable enough after one of these events that you find you have to reboot it to ensure that all of the system processes are running. If the OOM killer does kick in, you will see lines like the following in your /var/log/syslog: ([Location 676](https://readwise.io/to_kindle?action=open&asin=B00A4G7M06&location=676))
    - Tags: [[orange]] 
- When you see high I/O wait, one of the first things you should check is whether the machine is using a lot of swap. Since a hard drive is much slower than RAM, when a system runs out of RAM and starts using swap, the performance of almost any machine suffers. Anything that wants to access the disk has to compete with swap for disk I/O. So first diagnose whether you are out of memory and, if so, manage the problem there. If you do have plenty of RAM, you will need to figure out which program is consuming the most I/O. ([Location 685](https://readwise.io/to_kindle?action=open&asin=B00A4G7M06&location=685))
- partitions. Here is what each of the columns represents: • tps This lists the transfers per second to the device. “Transfers” is another way to say I/O requests sent to the device. • Blk_read/s This is the number of blocks read from the device per second. • Blk_wrtn/s This is the number of blocks written to the device per second. • Blk_read In this column is the total number of blocks read from the device. • Blk_wrtn In this column is the total number of blocks written to the device. When you have a system under heavy I/O load, the first step is to look at each of the partitions and identify which partition is getting the heaviest I/O load. Say, for instance, that you have a database server and the database itself is stored on /dev/sda3. If you see that the bulk of the I/O is coming from there, you have a good clue that the database is likely consuming the I/O. ([Location 709](https://readwise.io/to_kindle?action=open&asin=B00A4G7M06&location=709))
    - Tags: [[orange]] 
