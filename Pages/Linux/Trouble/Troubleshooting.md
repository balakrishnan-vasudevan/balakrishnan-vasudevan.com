
#linux

| Command           | Usage                                                                                                                                                                                                                                                                                                                                                              | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| uptime            | The three numbers are exponentially damped moving sum averages with a 1 minute, 5 minute, and 15 minute constant.                                                                                                                                                                                                                                                  | The three numbers give us some idea of how load is changing over time. For example, if you’ve been asked to check a problem server, and the 1 minute value is much lower than the 15 minute value, then you might have logged in too late and missed the issue.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| dmesg \| tail     | This views the last 10 system messages, if there are any.                                                                                                                                                                                                                                                                                                          | Look for errors that can cause performance issues. The example above includes the oom-killer, and TCP dropping a request.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| vmstat 1          | It prints a summary of key server statistics on each line. vmstat was run with an argument of 1, to print one second summaries. The first line of output (in this version of vmstat) has some columns that show the average since boot, instead of the previous second. For now, skip the first line, unless you want to learn and remember which column is which. | **r**: Number of processes running on CPU and waiting for a turn. This provides a better signal than load averages for determining CPU saturation, as it does not include I/O. To interpret: an “r” value greater than the CPU count is saturation.<br>- **free**: Free memory in kilobytes. If there are too many digits to count, you have enough free memory. The “free -m” command, included as command 7, better explains the state of free memory.<br>- **si, so**: Swap-ins and swap-outs. If these are non-zero, you’re out of memory.<br>- **us, sy, id, wa, st**: These are breakdowns of CPU time, on average across all CPUs. They are user time, system time (kernel), idle, wait I/O, and stolen time (by other guests, or with Xen, the guest’s own isolated driver domain).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| mpstat -P ALL 1   | This command prints CPU time breakdowns per CPU, which can be used to check for an imbalance.                                                                                                                                                                                                                                                                      | A single hot CPU can be evidence of a single-threaded application.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| pidstat 1         | Pidstat is a little like top’s per-process summary, but prints a rolling summary instead of clearing the screen.                                                                                                                                                                                                                                                   | This can be useful for watching patterns over time, and also recording what you saw (copy-n-paste) into a record of your investigation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| iostat -xz 1      | This is a great tool for understanding block devices (disks), both the workload applied and the resulting performance.                                                                                                                                                                                                                                             | - **r/s, w/s, rkB/s, wkB/s**: These are the delivered reads, writes, read Kbytes, and write Kbytes per second to the device. Use these for workload characterization. A performance problem may simply be due to an excessive load applied.<br>- **await**: The average time for the I/O in milliseconds. This is the time that the application suffers, as it includes both time queued and time being serviced. Larger than expected average times can be an indicator of device saturation, or device problems.<br>- **avgqu-sz**: The average number of requests issued to the device. Values greater than 1 can be evidence of saturation (although devices can typically operate on requests in parallel, especially virtual devices which front multiple back-end disks.)<br>- **%util**: Device utilization. This is really a busy percent, showing the time each second that the device was doing work. Values greater than 60% typically lead to poor performance (which should be seen in await), although it depends on the device. Values close to 100% usually indicate saturation.<br><br>If the storage device is a logical disk device fronting many back-end disks, then 100% utilization may just mean that some I/O is being processed 100% of the time, however, the back-end disks may be far from saturated, and may be able to handle much more work. |
| free -m           | The right two columns show:<br><br>- **buffers**: For the buffer cache, used for block device I/O.<br>- **cached**: For the page cache, used by file systems.                                                                                                                                                                                                      | We just want to check that these aren’t near-zero in size, which can lead to higher disk I/O (confirm using iostat), and worse performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| sar -n DEV 1      | Use this tool to check network interface throughput: rxkB/s and txkB/s, as a measure of workload, and also to check if any limit has been reached.                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| sar -n TCP,ETCP 1 | This is a summarized view of some key TCP metrics. These include:<br><br>- **active/s**: Number of locally-initiated TCP connections per second (e.g., via connect()).<br>- **passive/s**: Number of remotely-initiated TCP connections per second (e.g., via accept()).<br>- **retrans/s**: Number of TCP retransmits per second.                                 | The active and passive counts are often useful as a rough measure of server load: number of new accepted connections (passive), and number of downstream connections (active). It might help to think of active as outbound, and passive as inbound, but this isn’t strictly true (e.g., consider a localhost to localhost connection).<br><br>Retransmits are a sign of a network or server issue; it may be an unreliable network (e.g., the public Internet), or it may be due a server being overloaded and dropping packets. The example above shows just one new TCP connection per-second.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| top               | A downside to top is that it is harder to see patterns over time, which may be more clear in tools like vmstat and pidstat, which provide rolling output. Evidence of intermittent issues can also be lost if you don’t pause the output quick enough (Ctrl-S to pause, Ctrl-Q to continue), and the screen clears.                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |



![[Screenshot 2024-05-06 at 1.55.51 PM.png]]
![[Screenshot 2024-05-06 at 1.54.20 PM.png]]
![[Screenshot 2024-05-06 at 1.56.12 PM.png]]
![[Screenshot 2024-05-06 at 1.56.12 PM.png]]
## SRE Performance checklists 

## Linux Disk Checklist



1. **iostat -xz 1** ⟶ any disk I/O? if not, stop looking
2. **vmstat 1** ⟶ is this swapping? or, high sys time?
3. **df -h** ⟶ are file systems nearly full?
4. **ext4slower 10** ⟶ (zfs*, xfs*, etc.) slow file system I/O?
5. **bioslower 10** ⟶ if so, check disks
6. **ext4dist 1** ⟶ check distribution and rate
7. **biolatency 1** ⟶ if interesting, check disks
8. **cat /sys/devices/…/ioerr_cnt** ⟶ (if available) errors
9. **smartctl -l error /dev/sda1** ⟶ (if available) errors
## Linux Network Checklist

1. **sar -n DEV,EDEV 1** ⟶ at interface limits? or use nicstat
2. **sar -n TCP,ETCP 1** ⟶ active/passive load, retransmit rate
3. **cat /etc/resolv.conf** ⟶ it's always DNS
4. **mpstat -P ALL 1** ⟶ high kernel time? single hot CPU?
5. **tcpretrans** ⟶ what are the retransmits? state?
6. **tcpconnect** ⟶ connecting to anything unexpected?
7. **tcpaccept** ⟶ unexpected workload?
8. **netstat -rnv** ⟶ any inefficient routes?
9. **check firewall config** ⟶ anything blocking/throttling?
10. **netstat -s** ⟶ play 252 metric pickup

tcp*, are from bcc/BPF tools.

## Linux CPU Checklist

1. **uptime** ⟶ load averages
2. **vmstat 1** ⟶ system-wide utilization, run q length
3. **mpstat -P ALL 1** ⟶ CPU balance
4. **pidstat 1** ⟶ per-process CPU
5. **CPU flame graph** ⟶ CPU profiling
6. **CPU subsecond offset heat map** ⟶ look for gaps
7. **perf stat -a -- sleep 10** ⟶ IPC, LLC hit ratio

## High I/O
1. top command - 
   `|   |`
`|---|`
`|# top|`
`||top - 14:31:20 up 35 min, 4 users, load average: 2.25, 1.74, 1.68|`
`||Tasks: 71 total, 1 running, 70 sleeping, 0 stopped, 0 zombie|`
`||Cpu(s): 2.3%us, 1.7%sy, 0.0%ni, 0.0%id, 96.0%wa, 0.0%hi, 0.0%si, 0.0%st|`
`||Mem: 245440k total, 241004k used, 4436k free, 496k buffers|`
`||Swap: 409596k total, 5436k used, 404160k free, 182812k cached|`
Which the `top` man page defines as the **_“Amount of time the CPU has been waiting for I/O to complete.”_**
2. Find which disk is being written to using iostat
   `|   |`
`|---|`
`|$ iostat -x 2 5|`
`||avg-cpu: %user %nice %system %iowait %steal %idle|`
`||3.66 0.00 47.64 48.69 0.00 0.00|`
`|||`
`||Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util|`
`||sda 44.50 39.27 117.28 29.32 11220.94 13126.70 332.17 65.77 462.79 9.80 2274.71 7.60 111.41|`
`||dm-0 0.00 0.00 83.25 9.95 10515.18 4295.29 317.84 57.01 648.54 16.73 5935.79 11.48 107.02|`
`||dm-1 0.00 0.00 57.07 40.84 228.27 163.35 8.00 93.84 979.61 13.94 2329.08 10.93 107.02|`
Ignore the first line, that is from when the system was booted
In the above example, the percent utilized on disk `sda` is `111.41%` this is a good indicator that our problem lies with processes writing to `sda`.
While the test system in my example only has 1 disk, this type of information is beneficial when the server has multiple disks as this can narrow down the search for which process is utilizing I/O. 
Aside from percent utilized, there is a wealth of information in the output of `iostat`; items such as read and write requests per millisecond (`rrqm/s` & `wrqm/s`), reads and writes per second (`r/s` & `w/s`) and plenty more.
In our example, our program seems to be read and write heavy this information will be helpful when trying to identify the offending process.
3. Finding processes that are causing high I/O - use iotop
4. iotop is not installed. Check process state codes 
   - `D`: uninterruptible sleep (usually IO)
- `R`: running or runnable (on run queue)
- `S`: interruptible sleep (waiting for an event to complete)
- `T`: stopped, either by a job control signal or because it is being traced
- `W`: paging (not valid since the 2.6.xx kernel)
- `X`: dead (should never be seen)
- `Z`: defunct (“zombie”) process, terminated but not reaped by its parent
	Code D, get the process ID. Check /proc file for the process, to confirm io stats. The `read_bytes` and `write_bytes` are the number of bytes that this specific process has written and read from the storage layer.
5. Find which files are being written too heavily using "lsof -p <pid>".
`# lsof -p 16528`
 `COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME`
 `bonnie++ 16528 root cwd DIR 252,0 4096 130597 /tmp`
 `<truncated>`
 `bonnie++ 16528 root 8u REG 252,0 501219328 131869 /tmp/Bonnie.16528`
 `bonnie++ 16528 root 9u REG 252,0 501219328 131869 /tmp/Bonnie.16528`
 `bonnie++ 16528 root 10u REG 252,0 501219328 131869 /tmp/Bonnie.16528`
 `bonnie++ 16528 root 11u REG 252,0 501219328 131869 /tmp/Bonnie.16528`
 `bonnie++ 16528 root 12u REG 252,0 501219328 131869 /tmp/Bonnie.16528`
   6. Confirm that these files are the ones being written to, look at the /tmp file system and which disk it is on using df
      `# df /tmp`
 `Filesystem 1K-blocks Used Available Use% Mounted on`
 `/dev/mapper/workstation-root 7667140 2628608 4653920 37% /`
   7. From the o/p of the df command, identify the logical volume, in this case root.
   8. Using the `pvdisplay` command, we can see that the `/dev/sda5` partition part of the `sda` disk is the partition that the workstation volume group is using and, in turn, is where `/tmp` filesystem exists.
      `# pvdisplay`
  `--- Physical volume ---`
  `PV Name /dev/sda5`
  `VG Name workstation`
  `PV Size 7.76 GiB / not usable 2.00 MiB`
  `Allocatable yes`
  `PE Size 4.00 MiB`
  `Total PE 1986`
  `Free PE 8`
  `Allocated PE 1978`
  `PV UUID CLbABb-GcLB-l5z3-TCj3-IOK3-SQ2p-RDPW5S`
   9. Given this information, it is safe to say that the large files listed in the `lsof` output above is likely the files being read & written too frequently.

 
   